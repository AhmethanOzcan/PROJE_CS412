{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import os\n",
    "import tqdm\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import graphviz\n",
    "\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "#dev-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code(black_part, code_list, convo):\n",
    "    if len(black_part) > 0:\n",
    "        # First delete top bar\n",
    "        black_part.contents[0].decompose()\n",
    "        # Then extract code text\n",
    "        code = black_part.text\n",
    "        splitted_code = code.split(\"\\n\")\n",
    "        new_code = []\n",
    "        for i in splitted_code:\n",
    "            if len(i) != 0 and i[0] != \"#\":\n",
    "                new_code.append(i)\n",
    "        code = \"\\n\".join(new_code)\n",
    "                \n",
    "        code_list.append(code)\n",
    "        convo[0].contents[0].find_all(\"pre\")[0].decompose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/html/*.html\"\n",
    "\n",
    "code2convos = dict()\n",
    "\n",
    "pbar = tqdm.tqdm(sorted(list(glob(data_path))))\n",
    "for path in pbar:\n",
    "    # print(Path.cwd() / path)\n",
    "    file_code = os.path.basename(path).split(\".\")[0]\n",
    "    with open(path, \"r\", encoding=\"latin1\") as fh:\n",
    "            \n",
    "        # get the file id to use it as key later on\n",
    "        fid = os.path.basename(path).split(\".\")[0]\n",
    "\n",
    "        # read the html file\n",
    "        html_page = fh.read()\n",
    "\n",
    "        # parse the html file with bs4 so we can extract needed stuff\n",
    "        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "\n",
    "        # grab the conversations with the data-testid pattern\n",
    "        data_test_id_pattern = re.compile(r\"conversation-turn-[0-9]+\")\n",
    "        conversations = soup.find_all(\"div\", attrs={\"data-testid\": data_test_id_pattern})\n",
    "\n",
    "        convo_texts = []\n",
    "\n",
    "        for i, convo in enumerate(conversations):\n",
    "            convo = convo.find_all(\"div\", attrs={\"data-message-author-role\":re.compile( r\"[user|assistant]\") })\n",
    "            if len(convo) > 0:\n",
    "                # Search for code part of conversation\n",
    "                black_part = convo[0].find_all(\"div\", class_= \"bg-black rounded-md\")\n",
    "\n",
    "                role = convo[0].get(\"data-message-author-role\")\n",
    "                code = \"\"\n",
    "                # If there is a code part\n",
    "                if len(black_part) > 0:\n",
    "                    # First delete top bar\n",
    "                    black_part[0].contents[0].decompose()\n",
    "                    # Then extract code text\n",
    "                    code = black_part[0].text\n",
    "                    splitted_code = code.split(\"\\n\")\n",
    "                    new_code = []\n",
    "                    for i in splitted_code:\n",
    "                        if len(i) != 0 and i[0] != \"#\":\n",
    "                            new_code.append(i)\n",
    "                    code = \"\\n\".join(new_code)\n",
    "                    # At the end delete code from text to create seperate things\n",
    "                    convo[0].contents[0].find_all(\"pre\")[0].decompose()\n",
    "                \n",
    "                convo_texts.append({\n",
    "                        \"role\" : role,\n",
    "                        \"text\" : convo[0].text,\n",
    "                        \"code\" : code\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "        code2convos[file_code] = convo_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code2convos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see one of the conversations\n",
    "pprint(code2convos[\"f2f18684-4a16-4c05-a2d1-c0f96d1de869\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see one of the conversations\n",
    "pprint(code2convos[\"f2f18684-4a16-4c05-a2d1-c0f96d1de869\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text data before feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for later use\n",
    "def convert_list_to_str(my_list):\n",
    " \n",
    "    list_to_str = ' '.join([str(elem) for i,elem in enumerate(my_list)])\n",
    "    \n",
    "    return list_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    # Extract text content without HTML tags\n",
    "    clean_text = soup.get_text()\n",
    "\n",
    "    return clean_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for history, conversations in code2convos.items():\n",
    "    for conversation in conversations:\n",
    "        conversation[\"text\"] = remove_html_tags(conversation[\"text\"])\n",
    "        conversation[\"code\"] = [remove_html_tags(line) for line in conversation[\"code\"]]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    # Remove non-ASCII characters\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Replace multiple whitespaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "for code, convs in code2convos.items():\n",
    "    for c in convs:\n",
    "        c[\"text\"] = remove_non_ascii(c[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_chats = []\n",
    "for code, convs in code2convos.items():\n",
    "    if len(convs) == 0:\n",
    "        invalid_chats.append(code)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chat_id in invalid_chats:\n",
    "    print(\"deleting \", chat_id)\n",
    "    del code2convos[chat_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess scores dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the scores\n",
    "scores = pd.read_csv(\"scores.csv\", sep=\",\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = scores[scores.duplicated('code')]\n",
    "duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.drop_duplicates(subset=['code'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_to_drop = scores[scores['grade'].isna()].copy()\n",
    "print(row_to_drop)\n",
    "scores.dropna(subset=['grade'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del code2convos[\"56c6f8dd-f37c-44d2-9820-9459aa34c8af\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = scores[\"code\"].isin(invalid_chats)\n",
    "# delete those rows\n",
    "scores.drop(scores[condition].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to do:\n",
    "- Prompt matching with questions\n",
    "- Feature Engineering\n",
    "- Question Grades preparation\n",
    "- Train/Test split\n",
    "- Fitting a model for predicting the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Matching\n",
    "> We want to match the prompts with the questions in the Homework Let's\n",
    "> do it with a simple term frequency vectorizing method. For each prompt,\n",
    "> we will come with a vector that represents it. We will do the same\n",
    "> thing with each of the homework questions. Then, we will calculate the\n",
    "> vectors distanance to do the matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "code2prompts = defaultdict(list)\n",
    "for code , convos in code2convos.items():\n",
    "    user_prompts = []\n",
    "    for conv in convos:\n",
    "        if conv[\"role\"] == \"user\":\n",
    "            prompts.append(conv[\"text\"])\n",
    "            user_prompts.append(conv[\"text\"])\n",
    "    code2prompts[code] = user_prompts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"\"\"Initialize\n",
    "*   First make a copy of the notebook given to you as a starter.\n",
    "*   Make sure you choose Connect form upper right.\n",
    "*   You may upload the data to the section on your left on Colab, than right click on the .csv file and get the path of the file by clicking on \"Copy Path\". You will be using it when loading the data.\n",
    "\n",
    "\"\"\",\n",
    "#####################\n",
    "    \"\"\"Load training dataset (5 pts)\n",
    "    *  Read the .csv file with the pandas library\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Understanding the dataset & Preprocessing (15 pts)\n",
    "Understanding the Dataset: (5 pts)\n",
    "> - Find the shape of the dataset (number of samples & number of attributes). (Hint: You can use the **shape** function)\n",
    "> - Display variable names (both dependent and independent).\n",
    "> - Display the summary of the dataset. (Hint: You can use the **info** function)\n",
    "> - Display the first 5 rows from training dataset. (Hint: You can use the **head** function)\n",
    "Preprocessing: (10 pts)\n",
    "\n",
    "> - Check if there are any missing values in the dataset. If there are, you can either drop these values or fill it with most common values in corresponding rows. **Be careful that you have enough data for training the  model.**\n",
    "\n",
    "> - Encode categorical labels with the mappings given in the cell below. (Hint: You can use **map** function)\n",
    "\"\"\",\n",
    "\"\"\"Set X & y, split data (5 pts)\n",
    "\n",
    "*   Shuffle the dataset.\n",
    "*   Seperate your dependent variable X, and your independent variable y. The column health_metrics is y, the rest is X.\n",
    "*   Split training and test sets as 80% and 20%, respectively.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Features and Correlations (10 pts)\n",
    "\n",
    "* Correlations of features with health (4 points)\n",
    "Calculate the correlations for all features in dataset. Highlight any strong correlations with the target variable. Plot your results in a heatmap.\n",
    "\n",
    "* Feature Selection (3 points)\n",
    "Select a subset of features that are likely strong predictors, justifying your choices based on the computed correlations.\n",
    "\n",
    "* Hypothetical Driver Features (3 points)\n",
    "Propose two hypothetical features that could enhance the model's predictive accuracy for Y, explaining how they might be derived and their expected impact. Show the resulting correlations with target variable.\n",
    "\n",
    "* __Note:__ You get can get help from GPT.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Tune Hyperparameters (20 pts)\n",
    "* Choose 2 hyperparameters to tune. You can use the Scikit learn decision tree documentation for the available hyperparameters *(Hyperparameters are listed under \"Parameters\" in the documentation)*. Use GridSearchCV for hyperparameter tuning, with a cross-validation value of 5. Use validation accuracy to pick the best hyper-parameter values. (15 pts)\n",
    "-Explain the hyperparameters you chose to tune. *(What are the hyperparameters you chose? Why did you choose them?)* (5 pts)\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Re-train and plot the decision tree with the hyperparameters you have chosen (15 pts)\n",
    "- Re-train model with the hyperparameters you have chosen in part 5). (10 pts)\n",
    "- Plot the tree you have trained. (5 pts)\n",
    "Hint: You can import the **plot_tree** function from the sklearn library.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Test your classifier on the test set (20 pts)\n",
    "- Predict the labels of testing data using the tree you have trained in step 6. (10 pts)\n",
    "- Report the classification accuracy. (2 pts)\n",
    "- Plot & investigate the confusion matrix. Fill the following blanks. (8 pts)\n",
    "> The model most frequently mistakes class(es) _________ for class(es) _________.\n",
    "Hint: You can use the confusion_matrix function from sklearn.metrics\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Find the information gain on the first split (10 pts)\"\"\",\n",
    "#####################\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer = vectorizer.fit(prompts + questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_TF_IDF = pd.DataFrame(vectorizer.transform(questions).toarray(), columns=vectorizer.get_feature_names_out())\n",
    "questions_TF_IDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2prompts_tf_idf = dict()\n",
    "for code, user_prompts in code2prompts.items():\n",
    "    if len(user_prompts) == 0:\n",
    "        # some files have issues\n",
    "        print(code+\".html\")\n",
    "        continue\n",
    "    prompts_TF_IDF = pd.DataFrame(vectorizer.transform(user_prompts).toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    code2prompts_tf_idf[code] = prompts_TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2prompts_tf_idf[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2prompts_tf_idf[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code2prompts_tf_idf[\"f2f18684-4a16-4c05-a2d1-c0f96d1de869\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2prompts_tf_idf[\"f2f18684-4a16-4c05-a2d1-c0f96d1de869\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2cosine = dict()\n",
    "for code, user_prompts_tf_idf in code2prompts_tf_idf.items():\n",
    "    code2cosine[code] = pd.DataFrame(cosine_similarity(questions_TF_IDF,user_prompts_tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2questionmapping = dict()\n",
    "for code, cosine_scores in code2cosine.items():\n",
    "    code2questionmapping[code] = code2cosine[code].max(axis=1).tolist()\n",
    "\n",
    "\n",
    "question_mapping_scores = pd.DataFrame(code2questionmapping).T\n",
    "question_mapping_scores.reset_index(inplace=True)\n",
    "question_mapping_scores.rename(columns={i: f\"Q_{i}\" for i in range(len(questions))}, inplace=True)\n",
    "question_mapping_scores.rename(columns={\"index\" : \"code\"}, inplace=True)\n",
    "\n",
    "question_mapping_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Number of prompts that a uers asked\n",
    "- Number of complaints that a user makes e.g \"the code gives this error!\"\n",
    "- User prompts average number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2features = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "keywords2search = [\"error\", \"no\", \"thank\", \"next\", \"Entropy\"]\n",
    "keywords2search = [k.lower() for k in keywords2search]\n",
    "\n",
    "for code, convs in code2convos.items():\n",
    "    for c in convs:\n",
    "        text = c[\"text\"].lower()\n",
    "        if c[\"role\"] == \"user\":\n",
    "            # User Prompts\n",
    "\n",
    "            # count the user prompts\n",
    "            code2features[code][\"#user_prompts\"] += 1\n",
    "            \n",
    "            # count the keywords\n",
    "            for kw in keywords2search:\n",
    "                code2features[code][f\"#{kw}\"] +=  len(re.findall(rf\"\\b{kw}\\b\", text))\n",
    "\n",
    "            code2features[code][\"prompt_avg_chars\"] += len(text)\n",
    "        else:\n",
    "            # ChatGPT Responses\n",
    "            code2features[code][\"response_avg_chars\"] += len(text)\n",
    "\n",
    "        code2features[code][\"prompt_avg_chars\"] /= code2features[code][\"#user_prompts\"]   \n",
    "        code2features[code][\"response_avg_chars\"] /= code2features[code][\"#user_prompts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New features\n",
    "- #_hw_statements --> Useful to check if the student mentioned about the tass are for a homework in a Machine Learning course. ChatGPT may act accordingly when she knows the tasks will be presented in academy.\n",
    "\n",
    "- #_question_statements --> The way the student behaves might affect the responses of ChatGPT.\n",
    "\n",
    "- #_understand_statements --> If a students is eager to learn, it is more likely that she will use verbs like \"explain\". The more willingness to learn, the more likely to get a high grade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new features\n",
    "code2newfeatures = defaultdict(lambda : defaultdict(int))\n",
    "hw_statements = [\"cs412\", \"machine learning\", \"course\", \"ML\", \"homework\",\"412\",\"cs\", \"assignment\"]\n",
    "question_statements = [\"how\", \"why\", \"what\", \"where\", \"can you\", \"do you\"]\n",
    "understand_statements = [\"explain\", \"reason\", \"example\", \"instance\",\"demonstrate\",\"describe\", \"proof\" ,\"prove\",\"show\", \"I think\"]\n",
    "example_statements = [\"for example\", \"for instance\", \"ex:\", \"here is an example\", \"see this\", \"like this\", \"similar to\"]\n",
    "\n",
    "hw_statements = [k.lower() for k in hw_statements]\n",
    "question_statements = [q.lower() for q in question_statements]\n",
    "understand_statements = [u.lower() for u in understand_statements]\n",
    "example_statements = [n.lower() for n in example_statements]\n",
    "\n",
    "for code, convs in code2convos.items():\n",
    "    for c in convs:\n",
    "        text = c[\"text\"].lower()\n",
    "        if c[\"role\"] == \"user\":\n",
    "            # User Prompts\n",
    "                        \n",
    "            # count the keywords\n",
    "            for kw in hw_statements:\n",
    "                code2newfeatures[code][f\"#_hw_statements\"] +=  len(re.findall(rf\"\\b{kw}\\b\", text))\n",
    "            for qw in question_statements:\n",
    "                code2newfeatures[code][f\"#_question_statements\"] +=  len(re.findall(rf\"\\b{qw}\\b\", text))\n",
    "            for uw in understand_statements:\n",
    "                code2newfeatures[code][f\"#_understand_statements\"] +=  len(re.findall(rf\"\\b{uw}\\b\", text))\n",
    "            for nw in example_statements:\n",
    "                code2newfeatures[code][f\"#_example_statements\"] +=  len(re.findall(rf\"\\b{uw}\\b\", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge new features into features df\n",
    "for code, features in code2newfeatures.items():\n",
    "    for feature, value in features.items():\n",
    "        code2features[code][feature] += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(code2features).T\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Average code length char by char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count average code length in terms of number of chars in the code part of a response of ChatGPT\n",
    "\n",
    "for code, convs in code2convos.items():\n",
    "    num = 0\n",
    "    total_sum = 0\n",
    "    for c in convs:\n",
    "        if c[\"role\"] == \"assistant\":\n",
    "            num += 1\n",
    "            code_length_sum  = 0\n",
    "            for line in c[\"code\"]:\n",
    "                code_length_sum  += len(line)\n",
    "            total_sum += code_length_sum \n",
    "    code2features[code][\"avg_code_length\"] = (total_sum/num)\n",
    "    code2features[code][\"code_response_ratio\"] = code2features[code][\"avg_code_length\"] / code2features[code][\"response_avg_chars\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding number of imperative sentences given by the student\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load the English NLP model once\n",
    "\n",
    "def is_imperative(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Check for common characteristics of imperative sentences:\n",
    "    if len(doc) > 0:\n",
    "        first_token = doc[0]\n",
    "\n",
    "        # 1. Verb in imperative form at the beginning\n",
    "        if first_token.pos_ == \"VERB\" and first_token.tag_ in (\"VB\", \"VBP\"):\n",
    "\n",
    "            # 2. No subject or explicit subject\n",
    "            if not first_token.dep_ or first_token.dep_ not in (\"nsubj\", \"nsubjpass\"):\n",
    "\n",
    "                # 3. Handle exceptions and potential false positives:\n",
    "                if not (first_token.text.lower() in (\"let's\", \"let's\") and doc[1].dep_ == \"obj\"):  # Exclude \"Let's\" suggestions\n",
    "                    return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load the English NLP model once for efficiency\n",
    "\n",
    "\n",
    "for code, convos in code2convos.items():\n",
    "    imperative_sentence_count = 0\n",
    "\n",
    "    for conv in convos:\n",
    "        if conv[\"role\"] == \"user\":\n",
    "            text = conv[\"text\"]\n",
    "        \n",
    "\n",
    "            # Check for imperative sentences\n",
    "            sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\", text)  # Split text into sentences\n",
    "            for sentence in sentences:\n",
    "                if is_imperative(sentence):\n",
    "                    imperative_sentence_count += 1\n",
    "\n",
    "    code2features[code][\"imperative_sentence_count\"] =  imperative_sentence_count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for errors common word is \"Traceback\", we want to get # of errors that user dealt with\n",
    "\n",
    "for code, convos in code2convos.items():\n",
    "    traceback_count = 0\n",
    "\n",
    "    for conv in convos:\n",
    "        if conv[\"role\"] == \"user\":\n",
    "            text = conv[\"text\"]\n",
    "            traceback_count += text.count(\"Traceback\")  # Count occurrences in user text\n",
    "\n",
    "    code2features[code][\"#given_errors\"] = traceback_count\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(code2features).T\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for students who get negative information gain and try to solve it\n",
    "\n",
    "for code, convos in code2convos.items():\n",
    "    neg_info_gain = False\n",
    "\n",
    "    for conv in convos:\n",
    "        if conv[\"role\"] == \"user\":\n",
    "            text = conv[\"text\"]\n",
    "            if (text.count(\"negative\") > 0 and text.count(\"information gain\") > 0):\n",
    "                neg_info_gain = True\n",
    "           \n",
    "\n",
    "    code2features[code][\"is_info_gain_negative\"] = neg_info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- total number of sentences in prompts\n",
    "- average number of sentences in prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of sentences in a prompt\n",
    "\n",
    "def count_sentences(text):\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Return the count of sentences\n",
    "    return len(sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "\n",
    "text = \"\"\"My dataset is 3425x11. How should I set max_depth and min_samples_split lists? # param_grid represents the hyperparameters we want to try (our search space)\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 8, 12, 16],\n",
    "    'min_samples_split': [5, 8, 14, 20]\n",
    "}\n",
    "\n",
    "# estimator is the model we are evaluating, Decision Tree in our case\n",
    "estimator = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "\n",
    "# scoring is the score used to choose the best model\n",
    "scoring='accuracy'\n",
    "\n",
    "# cv is the number of folds to use for cross validation\n",
    "cv = 5\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    cv=cv)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\"\"\"\n",
    "\n",
    "sentence_count = count_sentences(text)\n",
    "print(f\"Number of sentences: {sentence_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Although the number of chars is quite high, the number of sentences is small. This is because the last part is actually a code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code, convs in code2convos.items():\n",
    "    sentence_count = 0\n",
    "    num = 0\n",
    "    for c in convs:\n",
    "        if c[\"role\"] == \"user\":\n",
    "            num += 1\n",
    "            sentence_count += count_sentences(c[\"text\"])\n",
    "    code2features[code][\"avg_sentences_in_prompts\"] = (sentence_count/num)\n",
    "    code2features[code][\"#sentences_in_prompts\"] = sentence_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(code2features).T\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We observed that some students provide the dataset to ChatGPT. We can check whether it has an impact on the grade. If it is provided, it is done in the top 10 prompts. Let's check them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_keywords = [\"Island where the penguin was found (Biscoe, Dream, Torgensen)\", \"cs412_hw1_dataset.csv\", \"https://www.kaggle.com/datasets/samybaladram/palmers-penguin-dataset-extended/data\", \"species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex,diet,life_stage,health_metrics,year\"]\n",
    "\n",
    "\n",
    "for code, convs in code2convos.items():\n",
    "    is_dataset_mentioned = any(\n",
    "        any(keyword in conv[\"text\"] for keyword in dataset_keywords)\n",
    "        for conv in convs[:10] if conv[\"role\"] == \"user\"\n",
    "    )\n",
    "    code2features[code][\"is_dataset_given\"] = is_dataset_mentioned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(code2features).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"is_dataset_given\"].value_counts())\n",
    "print(\"--------------------------------------\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking diverse similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the provided codes from ChatGPT to the students who took 100 points\n",
    "\n",
    "stu_w_100 = scores[scores[\"grade\"] == 100]\n",
    "stu_w_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stu_w_100_codes = {}\n",
    "for student, convs in code2convos.items():\n",
    "    if student in stu_w_100[\"code\"].values:\n",
    "        student_codes = []\n",
    "        for c in convs:\n",
    "            if c[\"role\"] == \"assistant\" and (len(c[\"code\"]) > 0):\n",
    "                student_codes.append(c[\"code\"])\n",
    "        stu_w_100_codes[student] = student_codes\n",
    "print(len(stu_w_100_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stu_codes = {}\n",
    "for student, convs in code2convos.items():\n",
    "    student_codes = []\n",
    "    for c in convs:\n",
    "        if c[\"role\"] == \"assistant\" and (len(c[\"code\"]) > 0):\n",
    "            student_codes.append(c[\"code\"])\n",
    "    stu_codes[student] = student_codes\n",
    "print(len(stu_codes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stu_codes[\"fb8de815-224c-4d06-9fd4-7156d1a9920d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stu_w_100_codes_list = []\n",
    "for student in stu_w_100_codes:\n",
    "    stu_w_100_codes_list.append(convert_list_to_str(stu_w_100_codes[student]))\n",
    "\n",
    "print(len(stu_w_100_codes_list))\n",
    "#print(stu_w_100_codes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_stu_w_100_codes = tfidf_vectorizer.fit_transform(stu_w_100_codes_list)\n",
    "\n",
    "code_responses_similarity = {}\n",
    "for student, code_respsonses in stu_codes.items():\n",
    "    cur_stu_tf_idf = tfidf_vectorizer.transform([convert_list_to_str(stu_codes[student])])\n",
    "    \n",
    "    # Calculate cosine similarity with the TF-IDF of conversations who scored 100\n",
    "    similarity_scores = cosine_similarity(cur_stu_tf_idf, tfidf_matrix_stu_w_100_codes)\n",
    "    \n",
    "    # We take the average similarity score for simplicity\n",
    "    code_responses_similarity[student] = similarity_scores.mean()\n",
    "\n",
    "similarity_with_stu_w_100_codes_df = pd.DataFrame.from_dict(code_responses_similarity, orient='index', columns=['similarity_with_stu_w_100_codes'])\n",
    "\n",
    "\n",
    "similarity_with_stu_w_100_codes_df = similarity_with_stu_w_100_codes_df.reset_index()\n",
    "\n",
    "similarity_with_stu_w_100_codes_df.columns = ['student', 'similarity_with_stu_w_100_codes']\n",
    "\n",
    "similarity_with_stu_w_100_codes_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores.drop(columns=['Unnamed: 0'], inplace=True)#Cause conflict when merging with df\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df = df.rename(columns={\"index\": \"code\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_df = pd.merge(df, scores, on='code', how=\"left\")\n",
    "temp_df.dropna(inplace=True)\n",
    "temp_df.drop_duplicates(\"code\",inplace=True, keep=\"first\")\n",
    "\n",
    "#temp_df=df\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playground for Similarity with the ones who got 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-ıdf vectorizer with only user prompts\n",
    "conversations_100 = []\n",
    "for code, convos in code2convos.items():\n",
    "    # Check if the code exists in temp_df and if the student scored 100\n",
    "    if code in temp_df['code'].values and temp_df.loc[temp_df['code'] == code, 'grade'].values[0] == 100:\n",
    "        convo_text = \" \".join(c[\"text\"] for c in convos if c[\"role\"] == \"user\")\n",
    "        conversations_100.append(convo_text)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_100 = tfidf_vectorizer.fit_transform(conversations_100)\n",
    "\n",
    "code2similarity = {}\n",
    "for code, convos in code2convos.items():\n",
    "    # Extract the text for the current student's conversation\n",
    "    student_convo = \" \".join(c[\"text\"] for c in convos if c[\"role\"] == \"user\")\n",
    "    \n",
    "    # Vectorize the current student's conversation using the TF-IDF model created earlier\n",
    "    student_tfidf = tfidf_vectorizer.transform([student_convo])\n",
    "    \n",
    "    # Calculate cosine similarity with the TF-IDF of conversations who scored 100\n",
    "    similarity_scores = cosine_similarity(student_tfidf, tfidf_matrix_100)\n",
    "    \n",
    "    # We take the average similarity score for simplicity\n",
    "    code2similarity[code] = similarity_scores.mean()\n",
    "\n",
    "# Step 4: Store these similarities in a new DataFrame\n",
    "similarity_df = pd.DataFrame.from_dict(code2similarity, orient='index', columns=['similarity_with_100'])\n",
    "\n",
    "\n",
    "similarity_df = similarity_df.reset_index()\n",
    "\n",
    "# Rename columns appropriately\n",
    "similarity_df.columns = ['code', 'similarity_with_100']\n",
    "\n",
    "# Now similarity_df_reset has a regular index and separate columns for 'code' and 'similarity_with_100'\n",
    "similarity_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-ıdf vectorizer with user and assistant convos\n",
    "conversations_100 = []\n",
    "for code, convos in code2convos.items():\n",
    "    # Check if the code exists in temp_df and if the student scored 100\n",
    "    if code in temp_df['code'].values and temp_df.loc[temp_df['code'] == code, 'grade'].values[0] == 100:\n",
    "        convo_text = \" \".join(c[\"text\"] for c in convos if c[\"role\"] == \"user\" or c[\"role\"] == \"assistant\")\n",
    "        conversations_100.append(convo_text)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_100 = tfidf_vectorizer.fit_transform(conversations_100)\n",
    "\n",
    "code2similarity = {}\n",
    "for code, convos in code2convos.items():\n",
    "    # Extract the text for the current student's conversation\n",
    "    student_convo = \" \".join(c[\"text\"] for c in convos if c[\"role\"] == \"user\" or c[\"role\"] == \"assistant\")\n",
    "    \n",
    "    # Vectorize the current student's conversation using the TF-IDF model created earlier\n",
    "    student_tfidf = tfidf_vectorizer.transform([student_convo])\n",
    "    \n",
    "    # Calculate cosine similarity with the TF-IDF of conversations who scored 100\n",
    "    similarity_scores = cosine_similarity(student_tfidf, tfidf_matrix_100)\n",
    "    \n",
    "    # We take the average similarity score for simplicity\n",
    "    code2similarity[code] = similarity_scores.mean()\n",
    "\n",
    "# Step 4: Store these similarities in a new DataFrame\n",
    "similarity_df2 = pd.DataFrame.from_dict(code2similarity, orient='index', columns=['similarity_with_100'])\n",
    "\n",
    "similarity_df2.head(100)\n",
    "\n",
    "similarity_df2 = similarity_df2.reset_index()\n",
    "\n",
    "# Rename columns appropriately\n",
    "similarity_df2.columns = ['code', 'similarity_with_100']\n",
    "\n",
    "# Now similarity_df_reset has a regular index and separate columns for 'code' and 'similarity_with_100'\n",
    "similarity_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Step 1: Prepare the data for Word2Vec\n",
    "# Tokenize the conversations\n",
    "tokenized_conversations = []\n",
    "for convos in code2convos.values():\n",
    "    for c in convos:\n",
    "        #if c['role'] == 'user' or c['role'] == 'assistant':  # Assuming you want to include only user's text\n",
    "            # Tokenize the text and add to the list\n",
    "        tokenized_conversations.append(c['text'].lower().split())\n",
    "\n",
    "# Step 2: Train a Word2Vec model\n",
    "# Here we are training a model on the tokenized conversations\n",
    "word2vec_model = Word2Vec(sentences=tokenized_conversations, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 3: Calculate the mean vector for conversations of students who scored 100\n",
    "# First, get the conversations for codes where the grade is 100\n",
    "codes_with_grade_100 = temp_df[temp_df['grade'] == 100]['code']\n",
    "conversations_100 = []\n",
    "\n",
    "for code in codes_with_grade_100:\n",
    "    convos = code2convos[code]\n",
    "    for c in convos:\n",
    "        if c['role'] == 'user':\n",
    "            conversations_100.extend(c['text'].lower().split())\n",
    "\n",
    "# Tokenize the conversations and filter out words not in the model's vocabulary\n",
    "conversations_100 = [word for word in conversations_100 if word in word2vec_model.wv]\n",
    "\n",
    "# Calculate the mean vector for these words\n",
    "mean_vector_100 = np.mean([word2vec_model.wv[word] for word in conversations_100], axis=0)\n",
    "\n",
    "# Step 4: Compare each student's conversation with the mean vector\n",
    "code2similarity = defaultdict(float)\n",
    "\n",
    "for code, convos in code2convos.items():\n",
    "    all_words = []\n",
    "    for c in convos:\n",
    "        if c['role'] == 'user':\n",
    "            all_words.extend(c['text'].lower().split())\n",
    "    \n",
    "    valid_words = [word for word in all_words if word in word2vec_model.wv]\n",
    "    \n",
    "    if not valid_words:  # Skip if there are no valid words\n",
    "        continue\n",
    "    \n",
    "    student_convo_vector = np.mean([word2vec_model.wv[word] for word in valid_words], axis=0)\n",
    "    \n",
    "    similarity_score = cosine_similarity([student_convo_vector], [mean_vector_100])[0][0]\n",
    "    \n",
    "    code2similarity[code] = similarity_score\n",
    "\n",
    "# Step 5: Convert the similarity scores to a DataFrame\n",
    "similarity_df_v2v = pd.DataFrame(list(code2similarity.items()), columns=['code', 'similarity_with_100_word2vec'])\n",
    "\n",
    "similarity_df_v2v.head(100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's compare different approaches for a data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the target code you want to print\n",
    "your_target_code = '089eb66d-4c3a-4f58-b98f-a3774a2efb34'\n",
    "\n",
    "# Locate the row with the target code in the DataFrame\n",
    "target_row = similarity_df.loc[similarity_df['code'] == your_target_code]\n",
    "target_row2 = similarity_df2.loc[similarity_df['code'] == your_target_code]\n",
    "target_rowv2v = similarity_df_v2v.loc[similarity_df['code'] == your_target_code]\n",
    "\n",
    "# Check if the target code exists in the DataFrame\n",
    "if not target_row.empty:\n",
    "    # Print the row containing the target code\n",
    "    print(target_row)\n",
    "    print(target_row2)\n",
    "    print(target_rowv2v)\n",
    "else:\n",
    "    print(\"Target code not found in the similarity_df.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores[\"code\"] = scores[\"code\"].apply(lambda x: x.strip())\n",
    "\n",
    "# selecting the columns we need and we care\n",
    "scores = scores[[\"code\", \"grade\"]]\n",
    "\n",
    "# show some examples\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check grades distribution\n",
    "\n",
    "plt.title('Histogram Grades')\n",
    "plt.hist(scores[\"grade\"], rwidth=.8, bins=np.arange(min(scores[\"grade\"]), max(scores[\"grade\"])+2) - 0.5)\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging scores with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.merge(temp_df, question_mapping_scores, on=\"code\", how=\"left\")\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge temp_df with similarity_df on the 'code' column\n",
    "\n",
    "#first update the column name\n",
    "similarity_df.rename(columns={'similarity_with_100': 'similarity_with_stu_w_100_prompts'}, inplace=True)\n",
    "temp_df = pd.merge(temp_df, similarity_df, on='code', how='left')\n",
    "\n",
    "# Display the first few rows of the merged dataframe\n",
    "temp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge temp_df with similarity_with_stu_w_100_codes_df on the 'code' column\n",
    "\n",
    "#first update the column name\n",
    "similarity_with_stu_w_100_codes_df.rename(columns={'student': 'code'}, inplace=True)\n",
    "temp_df = pd.merge(temp_df, similarity_with_stu_w_100_codes_df, on='code', how='left')\n",
    "\n",
    "# Display the first few rows of the merged dataframe\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling With Left Skewed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = temp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform = df.copy()\n",
    "df_transform['grade_log'] = np.log1p(df_transform['grade'])\n",
    "df_transform['grade_boxcox'], _ = boxcox(df_transform['grade']+1)\n",
    "df_transform['grade_sqrt'] = np.sqrt(df_transform['grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original and transformed distributions\n",
    "sns.histplot(df_transform['grade'], kde=True, label='Original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_transform['grade_log'], kde=True, label='Log-transformed')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_transform['grade_boxcox'], kde=True, label='Boxcox-transformed')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_transform['grade_sqrt'], kde=True, label='Sqrt-transformed')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The grade distribution is closer to normal distribution when Boxcox transformation is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'grade' in df with 'grade_boxcox' from df_transform\n",
    "df['grade'] = df_transform['grade_boxcox']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = shuffle(df, random_state=42)\n",
    "cols_to_drop = ['grade', \"code\"]\n",
    "X = df_shuffled.drop(cols_to_drop, axis=1)\n",
    "y = df_shuffled['grade']\n",
    "\n",
    "\n",
    "# Use stratified sampling in the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the resulting sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy df into check_df in order to transform obj->numeric\n",
    "check_df = df.copy()\n",
    "\n",
    "for col in check_df.columns:\n",
    "    # Convert columns with 'object' dtype to numeric\n",
    "    if check_df[col].dtype == 'object':\n",
    "        check_df[col] = pd.to_numeric(check_df[col], errors='coerce')\n",
    "\n",
    "# Now print the updated data types\n",
    "print(check_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping code because it cannot be turned into numeric\n",
    "check_df = check_df.drop(columns=['code'])\n",
    "check_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "correlation_matrix = check_df.corr()\n",
    "target_correlation = correlation_matrix['grade'].sort_values(ascending=False)\n",
    "\n",
    "# Plotting results in a heatmap\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "correlation_matrix = check_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "# Use a RobustScaler on the entire dataset\n",
    "robust_scaler = RobustScaler()\n",
    "X_scaled = robust_scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=30)  # Create PCA with a maximum of 30 components\n",
    "pca.fit(X_scaled)  # Fit PCA on the data\n",
    "exp_var_ratio = pca.explained_variance_ratio_  # Get explained variance ratios\n",
    "\n",
    "n_comps = 0\n",
    "count_exp_var = 0\n",
    "\n",
    "desired_var = 0.95\n",
    "# Loop to find the number of components needed to reach desired variance\n",
    "for i in exp_var_ratio:\n",
    "    n_comps += 1\n",
    "    count_exp_var += i\n",
    "    if count_exp_var >= desired_var:\n",
    "        break\n",
    "\n",
    "print(\"Number of components needed:\", n_comps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good news. PC1 achieves 0.95 of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training and testing sets\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters and values to tune for regression\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree Regressor\n",
    "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=dt_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search.fit(X_train, y_train)  # Replace X_train and y_train with your training data\n",
    "\n",
    "# Get the best hyperparameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score (Negative Mean Squared Error):\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gridsearch algorith bruteforced to use every paramather possible\n",
    "#also it calculates each scoring type in order to determine where our data is stronger\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "# Define the parameter grid for regression\n",
    "param_grid = {\n",
    "    'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2],\n",
    "    'max_features': [None, 'sqrt', 'log2', 0.25, 0.5, 0.75],\n",
    "    'random_state': [None, 42, 100],\n",
    "    'max_leaf_nodes': [None, 5, 10, 15],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2],\n",
    "    'ccp_alpha': [0.0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Create the model for regression\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Define the scoring methods\n",
    "scoring_methods = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n",
    "\n",
    "# Initialize a dictionary to store the best parameters and scores for each scoring method\n",
    "best_results = {method: {'params': None, 'score': float('-inf')} for method in scoring_methods}\n",
    "\n",
    "# Iterate through each scoring method\n",
    "for scoring_method in scoring_methods:\n",
    "    # Initialize best parameters and best score for the current scoring method\n",
    "    best_params = {}\n",
    "    best_score = float('-inf') if scoring_method != 'r2' else float('inf')  # For R², higher is better\n",
    "\n",
    "    # Iterate through each pair of hyperparameters\n",
    "    for param1, param2 in combinations(param_grid.keys(), 2):\n",
    "        param_set = {param1: param_grid[param1], param2: param_grid[param2]}\n",
    "        grid_search = GridSearchCV(model, param_set, cv=5, scoring=scoring_method)\n",
    "        grid_search.fit(X_train, y_train)  # Replace X_train, y_train with your training data\n",
    "        cv_score = grid_search.best_score_\n",
    "        \n",
    "        # Update best score and params if current score is better\n",
    "        is_better_score = cv_score > best_score if scoring_method != 'r2' else cv_score < best_score\n",
    "        if is_better_score:\n",
    "            best_score = cv_score\n",
    "            best_params = grid_search.best_params_\n",
    "\n",
    "    # Store the best results for the current scoring method\n",
    "    best_results[scoring_method] = {'params': best_params, 'score': best_score}\n",
    "\n",
    "# Print the best hyperparameters and scores for each scoring method\n",
    "for method, result in best_results.items():\n",
    "    print(f\"Best Parameters for {method}: {result['params']}\")\n",
    "    print(f\"Best Score for {method}: {result['score']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting and Analyzing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = DecisionTreeRegressor(random_state=0,criterion='squared_error', max_depth=10)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_MSEs = regressor.tree_.impurity   \n",
    "for idx, MSE in enumerate(regressor.tree_.impurity):\n",
    "    print(\"Node {} has MSE {}\".format(idx,MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['#user_prompts', '#error', '#no', '#thank', '#next', '#entropy',\n",
    "       'prompt_avg_chars', 'response_avg_chars', '#_hw_statements',\n",
    "       '#_question_statements', '#_understand_statements',\n",
    "       '#_example_statements', 'avg_code_length', 'code_response_ratio',\n",
    "       'imperative_sentence_count', '#given_errors', 'is_info_gain_negative',\n",
    "       'avg_sentences_in_prompts', '#sentences_in_prompts', 'is_dataset_given', 'Q_0', 'Q_1', 'Q_2', 'Q_3', 'Q_4', 'Q_5', 'Q_6', 'Q_7', 'Q_8',\n",
    "       'similarity_with_stu_w_100_prompts', 'similarity_with_stu_w_100_codes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Tree \n",
    "dot_data = tree.export_graphviz(regressor, out_file=None, feature_names=feature_names)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"hw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_train_pred = regressor.predict(X_train)\n",
    "y_test_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculation of Mean Squared Error (MSE)\n",
    "print(\"MSE Train:\", mean_squared_error(y_train,y_train_pred))\n",
    "print(\"MSE TEST:\", mean_squared_error(y_test,y_test_pred))\n",
    "\n",
    "print(\"R2 Train:\", r2_score(y_train,y_train_pred))\n",
    "print(\"R2 TEST:\", r2_score(y_test,y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boost and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# Initialize the GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=gb, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the parameters for the Gradient Boosting model\n",
    "params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 10,\n",
    "    'n_estimators': 200\n",
    "}\n",
    "\n",
    "# Create a Gradient Boosting Regressor model\n",
    "gb_model = GradientBoostingRegressor(**params)\n",
    "\n",
    "# Train the model\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error on Test Set: \", mse)\n",
    "print(\"R-squared (R2) Score on Test Set: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net Regression:\n",
    "\n",
    "    Combines L1 and L2 regularization. It helps when there are many correlated features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "\n",
    "#getting best hyperparameters\n",
    "# Define a range of values for alpha and l1_ratio\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "# Create an Elastic Net model\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(elastic_net, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Scale the features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "best_l1_ratio = grid_search.best_params_['l1_ratio']\n",
    "\n",
    "\n",
    "print(\"Best Hyperparameters: alpha =\", best_alpha, \", l1_ratio =\", best_l1_ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Elastic Net model\n",
    "elastic_net = ElasticNet(alpha=10.0, l1_ratio=0.7)  # You can adjust alpha and l1_ratio based on your needs\n",
    "\n",
    "# Train the model\n",
    "elastic_net.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = elastic_net.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error on Test Set: \", mse)\n",
    "print(\"R-squared (R2) Score on Test Set: \", r2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
