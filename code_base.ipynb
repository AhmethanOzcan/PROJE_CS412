{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import os\n",
    "import tqdm\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import graphviz\n",
    "\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:20<00:00,  6.23it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/html/*.html\"\n",
    "\n",
    "code2convos = dict()\n",
    "\n",
    "pbar = tqdm.tqdm(sorted(list(glob(data_path))))\n",
    "for path in pbar:\n",
    "    # print(Path.cwd() / path)\n",
    "    file_code = os.path.basename(path).split(\".\")[0]\n",
    "    with open(path, \"r\", encoding=\"latin1\") as fh:\n",
    "            \n",
    "        # get the file id to use it as key later on\n",
    "        fid = os.path.basename(path).split(\".\")[0]\n",
    "\n",
    "        # read the html file\n",
    "        html_page = fh.read()\n",
    "\n",
    "        # parse the html file with bs4 so we can extract needed stuff\n",
    "        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "\n",
    "        # grab the conversations with the data-testid pattern\n",
    "        data_test_id_pattern = re.compile(r\"conversation-turn-[0-9]+\")\n",
    "        conversations = soup.find_all(\"div\", attrs={\"data-testid\": data_test_id_pattern})\n",
    "\n",
    "        convo_texts = []\n",
    "\n",
    "        for i, convo in enumerate(conversations):\n",
    "            convo = convo.find_all(\"div\", attrs={\"data-message-author-role\":re.compile( r\"[user|assistant]\") })\n",
    "            if len(convo) > 0:\n",
    "                # Search for code part of conversation\n",
    "                black_part = convo[0].find_all(\"div\", class_= \"bg-black rounded-md\")\n",
    "\n",
    "                role = convo[0].get(\"data-message-author-role\")\n",
    "                code = \"\"\n",
    "                # If there is a code part\n",
    "                if len(black_part) > 0:\n",
    "                    # First delete top bar\n",
    "                    black_part[0].contents[0].decompose()\n",
    "                    # Then extract code text\n",
    "                    code = black_part[0].text\n",
    "                    splitted_code = code.split(\"\\n\")\n",
    "                    new_code = []\n",
    "                    for i in splitted_code:\n",
    "                        if len(i) != 0 and i[0] != \"#\":\n",
    "                            new_code.append(i)\n",
    "                    code = \"\\n\".join(new_code)\n",
    "                    # At the end delete code from text to create seperate things\n",
    "                    convo[0].contents[0].find_all(\"pre\")[0].decompose()\n",
    "                \n",
    "                convo_texts.append({\n",
    "                        \"role\" : role,\n",
    "                        \"text\" : convo[0].text,\n",
    "                        \"code\" : code          \n",
    "                    }\n",
    "                )\n",
    "                \n",
    "        code2convos[file_code] = convo_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': 'import pandas as pd\\n'\n",
      "         \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "         'df = pd.read_csv(file_path)\\n'\n",
      "         'print(df.head())',\n",
      " 'role': 'assistant',\n",
      " 'text': 'You can load a CSV file into a Pandas DataFrame in Python using the '\n",
      "         \"pandas.read_csv() function. Here's how you can do it for your \"\n",
      "         'specific CSV file:This code assumes that the CSV file '\n",
      "         \"'cs412_hw1_dataset.csv' is in the same directory as your Python \"\n",
      "         'script or Jupyter Notebook. If the file is in a different directory, '\n",
      "         'you can specify the full path to the file in the file_path '\n",
      "         'variable.The pd.read_csv() function will automatically infer the '\n",
      "         \"column names from the first row of the CSV file, so you don't need \"\n",
      "         'to explicitly specify the column names unless they are different '\n",
      "         'from what you provided. You can access columns in the DataFrame '\n",
      "         \"using their names, e.g., df['Species'], df['Island'], etc.\"}\n"
     ]
    }
   ],
   "source": [
    "# let's see one of the conversations\n",
    "pprint(code2convos[\"0031c86e-81f4-4eef-9e0e-28037abf9883\"][1])\n",
    "# pprint(code2convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'code': '',\n",
      "  'role': 'user',\n",
      "  'text': 'I have a csv file data with Columns:\\n'\n",
      "          '\\n'\n",
      "          'species: Species of the penguin (Adelie, Chinstrap, Gentoo)\\n'\n",
      "          '\\n'\n",
      "          'island: Island where the penguin was found (Biscoe, Dream, '\n",
      "          'Torgensen)\\n'\n",
      "          '\\n'\n",
      "          'sex: Gender of the penguin (Male, Female)\\n'\n",
      "          '\\n'\n",
      "          'diet: Primary diet of the penguin (Fish, Krill, Squid)\\n'\n",
      "          '\\n'\n",
      "          'year: Year the data was collected (2021-2025)\\n'\n",
      "          '\\n'\n",
      "          'life Stage: The life stage of the penguin (Chick, Juvenile, Adult)\\n'\n",
      "          '\\n'\n",
      "          'body_mass (g): Body mass in grams\\n'\n",
      "          '\\n'\n",
      "          'bill_length (mm): Bill length in millimeters\\n'\n",
      "          '\\n'\n",
      "          'bill_depth (mm): Bill depth in millimeters\\n'\n",
      "          '\\n'\n",
      "          'flipper_length (mm): Flipper length in millimeters\\n'\n",
      "          '\\n'\n",
      "          'health_metrics: Health status of the penguin (Healthy, Overweight, '\n",
      "          'Underweight)\\n'\n",
      "          '\\n'\n",
      "          \"It's named cs412_hw1_dataset.csv. You must use 20% of the data for \"\n",
      "          'test and 80% for training. Build a decision tree classifier with '\n",
      "          'the scikit library function to predict Penguin health conditions - '\n",
      "          'given in Target column health_metrics. But we will go step by step, '\n",
      "          'give me each step in a different code cell please. \\n'\n",
      "          '1) First of all import necessary libraries.\\n'\n",
      "          '2) Load training dataset: Read the .csv file with the pandas '\n",
      "          'library\\n'\n",
      "          '3) Understanding the dataset & Preprocessing:\\n'\n",
      "          'Understanding the Dataset: \\n'\n",
      "          '\\n'\n",
      "          'Find the shape of the dataset (number of samples & number of '\n",
      "          'attributes). (Hint: You can use the shape function)\\n'\n",
      "          'Display variable names (both dependent and independent).\\n'\n",
      "          'Display the summary of the dataset. (Hint: You can use the info '\n",
      "          'function)\\n'\n",
      "          'Display the first 5 rows from training dataset. (Hint: You can use '\n",
      "          'the head function)\\n'\n",
      "          'Preprocessing: (10 pts)\\n'\n",
      "          '\\n'\n",
      "          'Check if there are any missing values in the dataset. If there are, '\n",
      "          'you can either drop these values or fill it with most common values '\n",
      "          'in corresponding rows. Be careful that you have enough data for '\n",
      "          'training the model.\\n'\n",
      "          'Encode categorical labels with the mappings given in the code '\n",
      "          'below. (Hint: You can use map function)\\n'\n",
      "          'given mappings for the encode part:\\n'\n",
      "          \"sex_map = {'female':1, 'male': 0}\\n\"\n",
      "          '\\n'\n",
      "          \"island_map = {'Biscoe': 1,\\n\"\n",
      "          \"              'Dream': 2,\\n\"\n",
      "          \"              'Torgensen': 3}\\n\"\n",
      "          '\\n'\n",
      "          \"diet_map = {'fish': 1,\\n\"\n",
      "          \"              'krill': 2,\\n\"\n",
      "          \"              'squid': 3,\\n\"\n",
      "          '            \"parental\":4}\\n'\n",
      "          '\\n'\n",
      "          \"life_stage_map = {'chick': 1,\\n\"\n",
      "          \"              'juvenile': 2,\\n\"\n",
      "          \"              'adult': 3}\\n\"\n",
      "          '\\n'\n",
      "          \"health_metrics_map = {'healthy': 1,\\n\"\n",
      "          \"              'overweight': 2,\\n\"\n",
      "          \"              'underweight': 3}\\n\"\n",
      "          '\\n'\n",
      "          '# code here\\n'\n",
      "          '\\n'\n",
      "          '4) Set X & y, split data (5 pts)\\n'\n",
      "          'Shuffle the dataset.\\n'\n",
      "          'Seperate your dependent variable X, and your independent variable '\n",
      "          'y. The column health_metrics is y, the rest is X.\\n'\n",
      "          'Split training and test sets as 80% and 20%, respectively.\\n'\n",
      "          '\\n'\n",
      "          'given code for the 4 part complete this:\\n'\n",
      "          'from sklearn.utils import shuffle\\n'\n",
      "          'from sklearn.model_selection import train_test_split\\n'\n",
      "          '# complete the code\\n'\n",
      "          '\\n'\n",
      "          '4.1) Features and Correlations \\n'\n",
      "          'Correlations of features with health. Calculate the correlations '\n",
      "          'for all features in dataset. Highlight any strong correlations with '\n",
      "          'the target variable. Plot your results in a heatmap.\\n'\n",
      "          '\\n'\n",
      "          'Feature Selection. Select a subset of features that are likely '\n",
      "          'strong predictors, justifying your choices based on the computed '\n",
      "          'correlations.\\n'\n",
      "          '\\n'\n",
      "          'Hypothetical Driver Features. Propose two hypothetical features '\n",
      "          \"that could enhance the model's predictive accuracy for Y, \"\n",
      "          'explaining how they might be derived and their expected impact. '\n",
      "          'Show the resulting correlations with target variable.\\n'\n",
      "          '\\n'\n",
      "          '5) Tune Hyperparameters:\\n'\n",
      "          'Choose 2 hyperparameters to tune. You can use the Scikit learn '\n",
      "          'decision tree documentation for the available hyperparameters '\n",
      "          '(Hyperparameters are listed under \"Parameters\" in the '\n",
      "          'documentation). Use GridSearchCV for hyperparameter tuning, with a '\n",
      "          'cross-validation value of 5. Use validation accuracy to pick the '\n",
      "          'best hyper-parameter values.\\n'\n",
      "          'Explain the hyperparameters you chose to tune. (What are the '\n",
      "          'hyperparameters you chose? Why did you choose them?)\\n'\n",
      "          'given code for this part:\\n'\n",
      "          'from sklearn.tree import DecisionTreeClassifier\\n'\n",
      "          'from sklearn.metrics import accuracy_score\\n'\n",
      "          'from sklearn.model_selection import GridSearchCV\\n'\n",
      "          '# complete the code\\n'\n",
      "          '\\n'\n",
      "          '6) Re-train and plot the decision tree with the hyperparameters you '\n",
      "          'have chosen (15 pts)\\n'\n",
      "          'Re-train model with the hyperparameters you have chosen in part 5). '\n",
      "          '(10 pts)\\n'\n",
      "          'Plot the tree you have trained. (5 pts)\\n'\n",
      "          'Hint: You can import the plot_tree function from the sklearn '\n",
      "          'library.\\n'\n",
      "          '\\n'\n",
      "          '7) Test your classifier on the test set (20 pts)\\n'\n",
      "          'Predict the labels of testing data using the tree you have trained '\n",
      "          'in step 6. (10 pts)\\n'\n",
      "          'Report the classification accuracy. (2 pts)\\n'\n",
      "          'Plot & investigate the confusion matrix. Fill the following blanks. '\n",
      "          '(8 pts)\\n'\n",
      "          'The model most frequently mistakes class(es) _____ for class(es) '\n",
      "          '_____.\\n'\n",
      "          '\\n'\n",
      "          'Hint: You can use the confusion_matrix function from '\n",
      "          'sklearn.metrics\\n'\n",
      "          'given code for this part:\\n'\n",
      "          'from sklearn.metrics import confusion_matrix\\n'\n",
      "          'import matplotlib.pyplot as plt\\n'\n",
      "          'import seaborn as sns\\n'\n",
      "          '\\n'\n",
      "          'Fill the blanks: The model most frequently mistakes class(es) _____ '\n",
      "          'for class(es) _____.\\n'\n",
      "          '\\n'\n",
      "          '8) Find the information gain on the first split (10 pts)\\n'\n",
      "          'Find the information gain on the first split with Entropy according '\n",
      "          'to the formula from the lecture notes:\\n'\n",
      "          'Information gain: entropy(parent) - [average entropy(children)]'},\n",
      " {'code': 'import pandas as pd\\n'\n",
      "          'from sklearn.utils import shuffle\\n'\n",
      "          'from sklearn.model_selection import train_test_split\\n'\n",
      "          'from sklearn.tree import DecisionTreeClassifier, plot_tree\\n'\n",
      "          'from sklearn.metrics import accuracy_score, confusion_matrix\\n'\n",
      "          'import matplotlib.pyplot as plt\\n'\n",
      "          'import seaborn as sns\\n'\n",
      "          'from sklearn.model_selection import GridSearchCV',\n",
      "  'role': 'assistant',\n",
      "  'text': \"Certainly! Let's go through each step.Step 1: Import necessary \"\n",
      "          'librariesStep 2: Load training datasetpythonCopy code# Assuming the '\n",
      "          'dataset file is in the same directory as your script/notebook\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'data = pd.read_csv(file_path)\\n'\n",
      "          'Step 3: Understanding the dataset & PreprocessingUnderstanding the '\n",
      "          'DatasetpythonCopy code# Find the shape of the dataset\\n'\n",
      "          'print(\"Dataset shape:\", data.shape)\\n'\n",
      "          '\\n'\n",
      "          '# Display variable names\\n'\n",
      "          'print(\"Variable names:\", data.columns)\\n'\n",
      "          '\\n'\n",
      "          '# Display the summary of the dataset\\n'\n",
      "          'data.info()\\n'\n",
      "          '\\n'\n",
      "          '# Display the first 5 rows from the training dataset\\n'\n",
      "          'data.head()\\n'\n",
      "          'PreprocessingpythonCopy code# Check for missing values\\n'\n",
      "          'print(\"Missing values:\\\\n\", data.isnull().sum())\\n'\n",
      "          '\\n'\n",
      "          '# Fill missing values or drop rows if needed\\n'\n",
      "          '# Assuming filling with most common values in corresponding rows\\n'\n",
      "          'data.fillna(data.mode().iloc[0], inplace=True)\\n'\n",
      "          '\\n'\n",
      "          '# Encode categorical labels\\n'\n",
      "          \"sex_map = {'female': 1, 'male': 0}\\n\"\n",
      "          \"island_map = {'Biscoe': 1, 'Dream': 2, 'Torgensen': 3}\\n\"\n",
      "          'diet_map = {\\'fish\\': 1, \\'krill\\': 2, \\'squid\\': 3, \"parental\": '\n",
      "          '4}\\n'\n",
      "          \"life_stage_map = {'chick': 1, 'juvenile': 2, 'adult': 3}\\n\"\n",
      "          \"health_metrics_map = {'healthy': 1, 'overweight': 2, 'underweight': \"\n",
      "          '3}\\n'\n",
      "          '\\n'\n",
      "          \"data['sex'] = data['sex'].map(sex_map)\\n\"\n",
      "          \"data['island'] = data['island'].map(island_map)\\n\"\n",
      "          \"data['diet'] = data['diet'].map(diet_map)\\n\"\n",
      "          \"data['life_stage'] = data['life_stage'].map(life_stage_map)\\n\"\n",
      "          \"data['health_metrics'] = \"\n",
      "          \"data['health_metrics'].map(health_metrics_map)\\n\"\n",
      "          'Step 4: Set X & y, split datapythonCopy code# Shuffle the dataset\\n'\n",
      "          'data = shuffle(data, random_state=42)\\n'\n",
      "          '\\n'\n",
      "          '# Seperate dependent variable (y) and independent variables (X)\\n'\n",
      "          \"X = data.drop('health_metrics', axis=1)\\n\"\n",
      "          \"y = data['health_metrics']\\n\"\n",
      "          '\\n'\n",
      "          '# Split training and test sets as 80% and 20%\\n'\n",
      "          'X_train, X_test, y_train, y_test = train_test_split(X, y, '\n",
      "          'test_size=0.2, random_state=42)\\n'\n",
      "          'Step 4.1: Features and CorrelationspythonCopy code# Correlations of '\n",
      "          'features with health\\n'\n",
      "          'correlations = '\n",
      "          \"data.corr()['health_metrics'].sort_values(ascending=False)\\n\"\n",
      "          '\\n'\n",
      "          '# Plot correlations in a heatmap\\n'\n",
      "          'plt.figure(figsize=(10, 6))\\n'\n",
      "          'sns.heatmap(data.corr(), annot=True, cmap=\\'coolwarm\\', fmt=\".2f\", '\n",
      "          'linewidths=0.5)\\n'\n",
      "          \"plt.title('Correlation Heatmap')\\n\"\n",
      "          'plt.show()\\n'\n",
      "          '\\n'\n",
      "          '# Feature Selection: Select features with correlations greater than '\n",
      "          'a threshold\\n'\n",
      "          'selected_features = correlations[correlations.abs() > 0.1].index\\n'\n",
      "          'X_train_selected = X_train[selected_features]\\n'\n",
      "          'Step 5: Tune HyperparameterspythonCopy code# Choose hyperparameters '\n",
      "          'to tune\\n'\n",
      "          'param_grid = {\\n'\n",
      "          \"    'max_depth': [None, 10, 20, 30],\\n\"\n",
      "          \"    'min_samples_split': [2, 5, 10],\\n\"\n",
      "          '}\\n'\n",
      "          '\\n'\n",
      "          '# Create DecisionTreeClassifier\\n'\n",
      "          'dtree = DecisionTreeClassifier()\\n'\n",
      "          '\\n'\n",
      "          '# Use GridSearchCV for hyperparameter tuning\\n'\n",
      "          'grid_search = GridSearchCV(dtree, param_grid, cv=5, '\n",
      "          \"scoring='accuracy')\\n\"\n",
      "          'grid_search.fit(X_train_selected, y_train)\\n'\n",
      "          '\\n'\n",
      "          '# Get the best hyperparameters\\n'\n",
      "          'best_params = grid_search.best_params_\\n'\n",
      "          'print(\"Best Hyperparameters:\", best_params)\\n'\n",
      "          'Step 6: Re-train and plot the decision treepythonCopy code# '\n",
      "          'Re-train model with the best hyperparameters\\n'\n",
      "          'best_dtree = DecisionTreeClassifier(**best_params)\\n'\n",
      "          'best_dtree.fit(X_train_selected, y_train)\\n'\n",
      "          '\\n'\n",
      "          '# Plot the tree\\n'\n",
      "          'plt.figure(figsize=(20, 10))\\n'\n",
      "          'plot_tree(best_dtree, feature_names=X_train_selected.columns, '\n",
      "          'class_names=health_metrics_map.keys(), filled=True)\\n'\n",
      "          'plt.show()\\n'\n",
      "          'Step 7: Test your classifier on the test setpythonCopy code# '\n",
      "          'Predict labels on the test set\\n'\n",
      "          'y_pred = best_dtree.predict(X_test[selected_features])\\n'\n",
      "          '\\n'\n",
      "          '# Report classification accuracy\\n'\n",
      "          'accuracy = accuracy_score(y_test, y_pred)\\n'\n",
      "          'print(\"Classification Accuracy:\", accuracy)\\n'\n",
      "          '\\n'\n",
      "          '# Plot and investigate the confusion matrix\\n'\n",
      "          'cm = confusion_matrix(y_test, y_pred)\\n'\n",
      "          \"sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \"\n",
      "          'xticklabels=health_metrics_map.keys(), '\n",
      "          'yticklabels=health_metrics_map.keys())\\n'\n",
      "          \"plt.xlabel('Predicted')\\n\"\n",
      "          \"plt.ylabel('True')\\n\"\n",
      "          \"plt.title('Confusion Matrix')\\n\"\n",
      "          'plt.show()\\n'\n",
      "          '\\n'\n",
      "          '# Identify the most frequently mistaken classes\\n'\n",
      "          '# Replace the blanks with the appropriate class names based on your '\n",
      "          'confusion matrix\\n'\n",
      "          'print(\"The model most frequently mistakes class(es) _____ for '\n",
      "          'class(es) _____.\")\\n'\n",
      "          'Step 8: Find the information gain on the first splitpythonCopy '\n",
      "          'code# Assuming you have the entropy function available\\n'\n",
      "          'def entropy(labels):\\n'\n",
      "          '    # implementation of entropy calculation\\n'\n",
      "          '    pass\\n'\n",
      "          '\\n'\n",
      "          '# Calculate entropy for the parent node\\n'\n",
      "          'entropy_parent = entropy(y_train)\\n'\n",
      "          '\\n'\n",
      "          '# Calculate average entropy for the children nodes (considering the '\n",
      "          'first split)\\n'\n",
      "          '# You need to calculate the information gain using the formula '\n",
      "          'provided\\n'\n",
      "          'information_gain = entropy_parent - [average_entropy_children]\\n'\n",
      "          'print(\"Information Gain on the first split:\", information_gain)\\n'\n",
      "          'Make sure to replace the placeholders and customize the code '\n",
      "          'according to your needs.'},\n",
      " {'code': '',\n",
      "  'role': 'user',\n",
      "  'text': 'error occured in 4.1\\n'\n",
      "          '\\n'\n",
      "          'KeyError: \"[\\'health_metrics\\'] not in index\"'},\n",
      " {'code': 'selected_features = correlations[correlations.abs() > 0.1].index',\n",
      "  'role': 'assistant',\n",
      "  'text': 'I apologize for the oversight. It seems there was an error in the '\n",
      "          \"selection of features for correlation analysis. Let's correct \"\n",
      "          'it:Replace this part:with:pythonCopy code# Feature Selection: '\n",
      "          'Select features with correlations greater than a threshold\\n'\n",
      "          'selected_features = correlations[correlations.abs() > '\n",
      "          \"0.1].index.drop('health_metrics')\\n\"\n",
      "          \"This modification ensures that 'health_metrics' is not included as \"\n",
      "          \"a selected feature, as it's the target variable.\"},\n",
      " {'code': '',\n",
      "  'role': 'user',\n",
      "  'text': '4.1: Hypothetical Driver Features (3 points) Propose two '\n",
      "          \"hypothetical features that could enhance the model's predictive \"\n",
      "          'accuracy for Y, explaining how they might be derived and their '\n",
      "          'expected impact. Show the resulting correlations with target '\n",
      "          'variable.\\n'\n",
      "          '\\n'\n",
      "          'can you also answer this\\n'},\n",
      " {'code': '',\n",
      "  'role': 'assistant',\n",
      "  'text': 'Certainly! Here are two hypothetical features along with their '\n",
      "          'potential impact and expected correlations with the target '\n",
      "          'variable:Hypothetical Driver Feature 1: Daily Activity '\n",
      "          'LevelDerivation:\\n'\n",
      "          \"This feature could be derived from the penguin's daily movements \"\n",
      "          'and activities. Penguins are known for their different activity '\n",
      "          'levels based on factors like hunting, breeding, or resting. '\n",
      "          'Tracking their daily movements through observational studies or GPS '\n",
      "          'data could help quantify their activity level.Expected Impact:\\n'\n",
      "          'Penguins with higher daily activity levels might have better '\n",
      "          'overall health due to increased physical fitness and successful '\n",
      "          'foraging. On the other hand, sedentary behavior may be associated '\n",
      "          'with health issues.Expected Correlation:\\n'\n",
      "          'We would expect a positive correlation between the daily activity '\n",
      "          'level and the health metrics. Penguins with higher activity levels '\n",
      "          \"may be more likely to fall into the 'healthy' category.Hypothetical \"\n",
      "          'Driver Feature 2: Environmental Temperature ExposureDerivation:\\n'\n",
      "          \"This feature could be derived by measuring the penguins' exposure \"\n",
      "          'to different temperature ranges in their habitats. Penguins are '\n",
      "          'highly adapted to cold environments, and variations in temperature '\n",
      "          'could impact their metabolism and health.Expected Impact:\\n'\n",
      "          'Penguins exposed to extreme temperatures, either too high or too '\n",
      "          'low, might experience additional stress. This stress could impact '\n",
      "          'their overall health and susceptibility to diseases.Expected '\n",
      "          'Correlation:\\n'\n",
      "          'We would expect a non-linear correlation between the temperature '\n",
      "          'exposure and health metrics. Penguins exposed to moderate '\n",
      "          'temperatures might have higher health metrics, while those exposed '\n",
      "          'to extreme temperatures may show signs of stress and fall into the '\n",
      "          \"'underweight' or 'overweight' categories.Remember, these are \"\n",
      "          'hypothetical features, and their impact and correlation depend on '\n",
      "          'various factors and assumptions. The actual correlation can be '\n",
      "          \"determined by analyzing the data and studying the penguins' \"\n",
      "          'behavior in different environmental conditions.'},\n",
      " {'code': '',\n",
      "  'role': 'user',\n",
      "  'text': 'error in 6:\\n'\n",
      "          \"TypeError: 'dict_keys' object is not subscriptable\\n\"\n",
      "          '\\n'},\n",
      " {'code': 'plot_tree(best_dtree, feature_names=X_train_selected.columns, '\n",
      "          'class_names=health_metrics_map.keys(), filled=True)',\n",
      "  'role': 'assistant',\n",
      "  'text': 'Apologies for the oversight. The issue arises because the '\n",
      "          'class_names parameter in plot_tree function requires a list, but '\n",
      "          \"the health_metrics_map.keys() returns a dict_keys object. Let's \"\n",
      "          'correct that:Replace this line:with:pythonCopy '\n",
      "          'codeplot_tree(best_dtree, feature_names=X_train_selected.columns, '\n",
      "          'class_names=list(health_metrics_map.keys()), filled=True)\\n'\n",
      "          'This should resolve the error by converting the dict_keys object to '\n",
      "          'a list.'},\n",
      " {'code': '',\n",
      "  'role': 'user',\n",
      "  'text': 'in part 7 can you fill the blanks in the sentence: The model most '\n",
      "          'frequently mistakes class(es) _____ for class(es) _____.'},\n",
      " {'code': '                Predicted\\n'\n",
      "          '               |  Class A |  Class B |  Class C |\\n'\n",
      "          'Actual Class A |    TP    |    FN    |    FN    |\\n'\n",
      "          'Actual Class B |    FP    |    TP    |    FN    |\\n'\n",
      "          'Actual Class C |    FN    |    FP    |    TP    |',\n",
      "  'role': 'assistant',\n",
      "  'text': \"Certainly! To fill in the blanks, you'll need to analyze the \"\n",
      "          'confusion matrix and identify the class(es) that are most '\n",
      "          \"frequently misclassified. Let's assume you have a confusion matrix \"\n",
      "          '(cm). The confusion matrix typically looks like this:Here, TP is '\n",
      "          'the true positive count, FN is the false negative count, and FP is '\n",
      "          'the false positive count.The model most frequently mistakes '\n",
      "          'class(es) _____ for class(es) _____ can be filled by identifying '\n",
      "          'which elements in the confusion matrix are relatively higher. For '\n",
      "          'example:pythonCopy code# Identify the class(es) most frequently '\n",
      "          'mistaken for another class\\n'\n",
      "          'most_mistaken_class = None\\n'\n",
      "          'for i in range(len(cm)):\\n'\n",
      "          '    for j in range(len(cm)):\\n'\n",
      "          '        if i != j and (most_mistaken_class is None or cm[i, j] > '\n",
      "          'cm[most_mistaken_class[0], most_mistaken_class[1]]):\\n'\n",
      "          '            most_mistaken_class = (i, j)\\n'\n",
      "          '\\n'\n",
      "          '# Fill in the blanks\\n'\n",
      "          'print(f\"The model most frequently mistakes class(es) '\n",
      "          '{most_mistaken_class[1]} for class(es) {most_mistaken_class[0]}.\")\\n'\n",
      "          'This code will identify the classes that are most frequently '\n",
      "          'mistaken and fill in the blanks accordingly.'},\n",
      " {'code': '',\n",
      "  'role': 'user',\n",
      "  'text': 'according to the code you gave me for the 8th part has an error\\n'\n",
      "          \"NameError: name 'average_entropy_children' is not defined\\n\"},\n",
      " {'code': 'def entropy(labels):\\n'\n",
      "          '    # implementation of entropy calculation\\n'\n",
      "          '    pass\\n'\n",
      "          'entropy_parent = entropy(y_train)\\n'\n",
      "          \"selected_feature = 'your_selected_feature'\\n\"\n",
      "          'left_child = X_train[selected_feature] <= '\n",
      "          'X_train[selected_feature].median()\\n'\n",
      "          'right_child = ~left_child\\n'\n",
      "          'entropy_left_child = entropy(y_train[left_child])\\n'\n",
      "          'entropy_right_child = entropy(y_train[right_child])\\n'\n",
      "          'average_entropy_children = (sum(left_child) / len(y_train)) * '\n",
      "          'entropy_left_child + (sum(right_child) / len(y_train)) * '\n",
      "          'entropy_right_child\\n'\n",
      "          'information_gain = entropy_parent - average_entropy_children\\n'\n",
      "          'print(\"Information Gain on the first split:\", information_gain)',\n",
      "  'role': 'assistant',\n",
      "  'text': 'I apologize for the confusion. It seems there was a placeholder '\n",
      "          'left in the code. The calculation for average entropy requires '\n",
      "          'splitting the data based on a feature and calculating the entropy '\n",
      "          \"for each child node. Here's the corrected code:Make sure to replace \"\n",
      "          \"'your_selected_feature' with the actual feature you want to \"\n",
      "          'analyze. Also, ensure that you have a function entropy() that '\n",
      "          \"calculates the entropy of a set of labels. If you haven't defined \"\n",
      "          \"such a function, you'll need to implement it based on the entropy \"\n",
      "          'calculation method you prefer (e.g., Shannon entropy).'},\n",
      " {'code': '',\n",
      "  'role': 'user',\n",
      "  'text': \"NameError: name 'y_train' is not defined\"},\n",
      " {'code': 'def entropy(labels):\\n'\n",
      "          '    # implementation of entropy calculation\\n'\n",
      "          '    pass\\n'\n",
      "          'entropy_parent = entropy(y_train)\\n'\n",
      "          \"selected_feature = 'your_selected_feature'\\n\"\n",
      "          'left_child = X_train[selected_feature] <= '\n",
      "          'X_train[selected_feature].median()\\n'\n",
      "          'right_child = ~left_child\\n'\n",
      "          'entropy_left_child = entropy(y_train[left_child])\\n'\n",
      "          'entropy_right_child = entropy(y_train[right_child])\\n'\n",
      "          'average_entropy_children = (sum(left_child) / len(y_train)) * '\n",
      "          'entropy_left_child + (sum(right_child) / len(y_train)) * '\n",
      "          'entropy_right_child\\n'\n",
      "          'information_gain = entropy_parent - average_entropy_children\\n'\n",
      "          'print(\"Information Gain on the first split:\", information_gain)',\n",
      "  'role': 'assistant',\n",
      "  'text': 'I apologize for the confusion. It appears that I overlooked '\n",
      "          \"defining y_train before using it in the example. Let's correct \"\n",
      "          \"that:Replace 'your_selected_feature' with the actual feature you \"\n",
      "          'want to analyze. Additionally, ensure that you have a function '\n",
      "          'entropy() that calculates the entropy of a set of labels. If you '\n",
      "          \"haven't defined such a function, you'll need to implement it based \"\n",
      "          'on the entropy calculation method you prefer (e.g., Shannon '\n",
      "          'entropy).'},\n",
      " {'code': '',\n",
      "  'role': 'user',\n",
      "  'text': 'average_entropy_children = (sum(left_child) / len(y_train)) * '\n",
      "          'entropy_left_child + (sum(right_child) / len(y_train)) * '\n",
      "          'entropy_right_child\\n'\n",
      "          \"TypeError: unsupported operand type(s) for *: 'float' and \"\n",
      "          \"'NoneType'\"},\n",
      " {'code': 'def entropy(labels):\\n'\n",
      "          '    unique_labels, label_counts = np.unique(labels, '\n",
      "          'return_counts=True)\\n'\n",
      "          '    probabilities = label_counts / len(labels)\\n'\n",
      "          '    entropy_value = -np.sum(probabilities * '\n",
      "          'np.log2(probabilities))\\n'\n",
      "          '    return entropy_value\\n'\n",
      "          \"selected_feature = 'your_selected_feature'\\n\"\n",
      "          'left_child = X_train[selected_feature] <= '\n",
      "          'X_train[selected_feature].median()\\n'\n",
      "          'right_child = ~left_child\\n'\n",
      "          'entropy_parent = entropy(y_train)\\n'\n",
      "          'entropy_left_child = entropy(y_train[left_child])\\n'\n",
      "          'entropy_right_child = entropy(y_train[right_child])\\n'\n",
      "          'average_entropy_children = (sum(left_child) / len(y_train)) * '\n",
      "          'entropy_left_child + (sum(right_child) / len(y_train)) * '\n",
      "          'entropy_right_child\\n'\n",
      "          'information_gain = entropy_parent - average_entropy_children\\n'\n",
      "          'print(\"Information Gain on the first split:\", information_gain)',\n",
      "  'role': 'assistant',\n",
      "  'text': 'I apologize for the oversight. It seems there might be an issue '\n",
      "          'with the calculation of entropy for the child nodes. Please ensure '\n",
      "          'that your entropy function returns a numeric value. Also, make sure '\n",
      "          'to check if there are any issues with the selected feature and if '\n",
      "          \"it indeed splits the data.Here's a revised version that includes a \"\n",
      "          'simple example of an entropy function:Ensure that your entropy '\n",
      "          \"function is returning valid numeric values, and you've replaced \"\n",
      "          \"'your_selected_feature' with the actual feature you want to \"\n",
      "          'analyze. If you encounter further issues, please share the '\n",
      "          \"implementation of your entropy function, and I'll be happy to \"\n",
      "          'assist further.'},\n",
      " {'code': '', 'role': 'user', 'text': 'thank you so much\\n'},\n",
      " {'code': '',\n",
      "  'role': 'assistant',\n",
      "  'text': \"You're welcome! If you have any more questions or if there's \"\n",
      "          'anything else I can help you with, feel free to ask. Happy coding!'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(code2convos[\"041f950b-c013-409a-a642-cffff60b9d4b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get word counts for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Use a simple regex to split words and lowercasing for normalization\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "def preprocess_conversations(code2convos):\n",
    "    all_texts = []\n",
    "    for conversations in code2convos.values():\n",
    "        combined_text = \"\"\n",
    "        for convo in conversations:\n",
    "            role_prefix = \"user_\" if convo.get('role') == 'user' else \"system_\"\n",
    "            text = convo.get('text', '')\n",
    "            code = convo.get('code', '')\n",
    "            \n",
    "            # Add prefix to each word\n",
    "            prefixed_text = ' '.join([role_prefix + word for word in tokenize(text)])\n",
    "            prefixed_code = ' '.join([role_prefix + word for word in tokenize(code)])\n",
    "            \n",
    "            combined_text += \" \" + prefixed_text + \" \" + prefixed_code\n",
    "        all_texts.append(combined_text.strip())\n",
    "    return all_texts\n",
    "\n",
    "all_texts = preprocess_conversations(code2convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   system_0  system_00  system_000  system_000000  system_000284among  \\\n",
      "0  0.028938        0.0         0.0            0.0                 0.0   \n",
      "1  0.025373        0.0         0.0            0.0                 0.0   \n",
      "2  0.042181        0.0         0.0            0.0                 0.0   \n",
      "3  0.041106        0.0         0.0            0.0                 0.0   \n",
      "4  0.043533        0.0         0.0            0.0                 0.0   \n",
      "\n",
      "   system_00075  system_000750  system_000750â  system_001  system_01  ...  \\\n",
      "0           0.0            0.0             0.0         0.0        0.0  ...   \n",
      "1           0.0            0.0             0.0         0.0        0.0  ...   \n",
      "2           0.0            0.0             0.0         0.0        0.0  ...   \n",
      "3           0.0            0.0             0.0         0.0        0.0  ...   \n",
      "4           0.0            0.0             0.0         0.0        0.0  ...   \n",
      "\n",
      "   user_zero  user_zerodivisionerror  user_zeroth  user_zip  user_â    user_ã  \\\n",
      "0        0.0                     0.0          0.0  0.000000     0.0  0.000000   \n",
      "1        0.0                     0.0          0.0  0.000000     0.0  0.000000   \n",
      "2        0.0                     0.0          0.0  0.000000     0.0  0.000000   \n",
      "3        0.0                     0.0          0.0  0.000000     0.0  0.000000   \n",
      "4        0.0                     0.0          0.0  0.002817     0.0  0.002406   \n",
      "\n",
      "   user_ã¼nã¼yorum  user_ä  user_å  grade  \n",
      "0              0.0     0.0     0.0   90.0  \n",
      "1              0.0     0.0     0.0   97.0  \n",
      "2              0.0     0.0     0.0   94.0  \n",
      "3              0.0     0.0     0.0   97.0  \n",
      "4              0.0     0.0     0.0   93.0  \n",
      "\n",
      "[5 rows x 15090 columns]\n"
     ]
    }
   ],
   "source": [
    "# reading the scores\n",
    "scores = pd.read_csv(\"scores.csv\", sep=\",\")\n",
    "\n",
    "# Merge the TF-IDF DataFrame with the grades\n",
    "final_df = pd.concat([tfidf_df, scores['grade']], axis=1)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to do:\n",
    "- Prompt matching with questions\n",
    "- Feature Engineering\n",
    "- Question Grades preparation\n",
    "- Train/Test split\n",
    "- Fitting a model for predicting the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Matching\n",
    "> We want to match the prompts with the questions in the Homework Let's\n",
    "> do it with a simple term frequency vectorizing method. For each prompt,\n",
    "> we will come with a vector that represents it. We will do the same\n",
    "> thing with each of the homework questions. Then, we will calculate the\n",
    "> vectors distanance to do the matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "code2prompts = defaultdict(list)\n",
    "for code , convos in code2convos.items():\n",
    "    user_prompts = []\n",
    "    for conv in convos:\n",
    "        if conv[\"role\"] == \"user\":\n",
    "            prompts.append(conv[\"text\"])\n",
    "            user_prompts.append(conv[\"text\"])\n",
    "    code2prompts[code] = user_prompts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Load a CSV file into a Pandas in Python. The file is named 'cs412_hw1_dataset.csv' and contains columns like 'Species', 'Island', 'Sex', 'Diet', 'Year', 'Life Stage', 'Body Mass (g)', 'Bill Length (mm)', 'Bill Depth (mm)', 'Flipper Length (mm)', and 'Health Metrics'. \\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"\"\"Initialize\n",
    "*   First make a copy of the notebook given to you as a starter.\n",
    "*   Make sure you choose Connect form upper right.\n",
    "*   You may upload the data to the section on your left on Colab, than right click on the .csv file and get the path of the file by clicking on \"Copy Path\". You will be using it when loading the data.\n",
    "\n",
    "\"\"\",\n",
    "#####################\n",
    "    \"\"\"Load training dataset (5 pts)\n",
    "    *  Read the .csv file with the pandas library\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Understanding the dataset & Preprocessing (15 pts)\n",
    "Understanding the Dataset: (5 pts)\n",
    "> - Find the shape of the dataset (number of samples & number of attributes). (Hint: You can use the **shape** function)\n",
    "> - Display variable names (both dependent and independent).\n",
    "> - Display the summary of the dataset. (Hint: You can use the **info** function)\n",
    "> - Display the first 5 rows from training dataset. (Hint: You can use the **head** function)\n",
    "Preprocessing: (10 pts)\n",
    "\n",
    "> - Check if there are any missing values in the dataset. If there are, you can either drop these values or fill it with most common values in corresponding rows. **Be careful that you have enough data for training the  model.**\n",
    "\n",
    "> - Encode categorical labels with the mappings given in the cell below. (Hint: You can use **map** function)\n",
    "\"\"\",\n",
    "\"\"\"Set X & y, split data (5 pts)\n",
    "\n",
    "*   Shuffle the dataset.\n",
    "*   Seperate your dependent variable X, and your independent variable y. The column health_metrics is y, the rest is X.\n",
    "*   Split training and test sets as 80% and 20%, respectively.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Features and Correlations (10 pts)\n",
    "\n",
    "* Correlations of features with health (4 points)\n",
    "Calculate the correlations for all features in dataset. Highlight any strong correlations with the target variable. Plot your results in a heatmap.\n",
    "\n",
    "* Feature Selection (3 points)\n",
    "Select a subset of features that are likely strong predictors, justifying your choices based on the computed correlations.\n",
    "\n",
    "* Hypothetical Driver Features (3 points)\n",
    "Propose two hypothetical features that could enhance the model's predictive accuracy for Y, explaining how they might be derived and their expected impact. Show the resulting correlations with target variable.\n",
    "\n",
    "* __Note:__ You get can get help from GPT.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Tune Hyperparameters (20 pts)\n",
    "* Choose 2 hyperparameters to tune. You can use the Scikit learn decision tree documentation for the available hyperparameters *(Hyperparameters are listed under \"Parameters\" in the documentation)*. Use GridSearchCV for hyperparameter tuning, with a cross-validation value of 5. Use validation accuracy to pick the best hyper-parameter values. (15 pts)\n",
    "-Explain the hyperparameters you chose to tune. *(What are the hyperparameters you chose? Why did you choose them?)* (5 pts)\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Re-train and plot the decision tree with the hyperparameters you have chosen (15 pts)\n",
    "- Re-train model with the hyperparameters you have chosen in part 5). (10 pts)\n",
    "- Plot the tree you have trained. (5 pts)\n",
    "Hint: You can import the **plot_tree** function from the sklearn library.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Test your classifier on the test set (20 pts)\n",
    "- Predict the labels of testing data using the tree you have trained in step 6. (10 pts)\n",
    "- Report the classification accuracy. (2 pts)\n",
    "- Plot & investigate the confusion matrix. Fill the following blanks. (8 pts)\n",
    "> The model most frequently mistakes class(es) _________ for class(es) _________.\n",
    "Hint: You can use the confusion_matrix function from sklearn.metrics\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Find the information gain on the first split (10 pts)\"\"\",\n",
    "#####################\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer = vectorizer.fit(prompts + questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00000</th>\n",
       "      <th>000000</th>\n",
       "      <th>000196</th>\n",
       "      <th>000282</th>\n",
       "      <th>000284</th>\n",
       "      <th>000360</th>\n",
       "      <th>000469</th>\n",
       "      <th>000750</th>\n",
       "      <th>000886</th>\n",
       "      <th>000991</th>\n",
       "      <th>...</th>\n",
       "      <th>yticks</th>\n",
       "      <th>yã</th>\n",
       "      <th>yä</th>\n",
       "      <th>zaman</th>\n",
       "      <th>zdã¼m</th>\n",
       "      <th>zero</th>\n",
       "      <th>zerodivisionerror</th>\n",
       "      <th>zeroth</th>\n",
       "      <th>zip</th>\n",
       "      <th>ã¼nã¼yorum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00000  000000  000196  000282  000284  000360  000469  000750  000886  \\\n",
       "0    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   000991  ...  yticks   yã   yä  zaman  zdã¼m  zero  zerodivisionerror  \\\n",
       "0     0.0  ...     0.0  0.0  0.0    0.0    0.0   0.0                0.0   \n",
       "1     0.0  ...     0.0  0.0  0.0    0.0    0.0   0.0                0.0   \n",
       "2     0.0  ...     0.0  0.0  0.0    0.0    0.0   0.0                0.0   \n",
       "3     0.0  ...     0.0  0.0  0.0    0.0    0.0   0.0                0.0   \n",
       "4     0.0  ...     0.0  0.0  0.0    0.0    0.0   0.0                0.0   \n",
       "\n",
       "   zeroth  zip  ã¼nã¼yorum  \n",
       "0     0.0  0.0         0.0  \n",
       "1     0.0  0.0         0.0  \n",
       "2     0.0  0.0         0.0  \n",
       "3     0.0  0.0         0.0  \n",
       "4     0.0  0.0         0.0  \n",
       "\n",
       "[5 rows x 5400 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_TF_IDF = pd.DataFrame(vectorizer.transform(questions).toarray(), columns=vectorizer.get_feature_names_out())\n",
    "questions_TF_IDF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139235c7-736c-4237-92f0-92e8c116832c.html\n",
      "668ad17e-0240-49f7-b5a7-d22e502554c6.html\n",
      "b0640e51-6879-40cb-a4f5-329f952ef99d.html\n",
      "da6b70d5-29f6-491a-ad46-037c77067128.html\n"
     ]
    }
   ],
   "source": [
    "code2prompts_tf_idf = dict()\n",
    "for code, user_prompts in code2prompts.items():\n",
    "    if len(user_prompts) == 0:\n",
    "        # some files have issues\n",
    "        print(code+\".html\")\n",
    "        continue\n",
    "    prompts_TF_IDF = pd.DataFrame(vectorizer.transform(user_prompts).toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    code2prompts_tf_idf[code] = prompts_TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00000</th>\n",
       "      <th>000000</th>\n",
       "      <th>000196</th>\n",
       "      <th>000282</th>\n",
       "      <th>000284</th>\n",
       "      <th>000360</th>\n",
       "      <th>000469</th>\n",
       "      <th>000750</th>\n",
       "      <th>000886</th>\n",
       "      <th>000991</th>\n",
       "      <th>...</th>\n",
       "      <th>yticks</th>\n",
       "      <th>yã</th>\n",
       "      <th>yä</th>\n",
       "      <th>zaman</th>\n",
       "      <th>zdã¼m</th>\n",
       "      <th>zero</th>\n",
       "      <th>zerodivisionerror</th>\n",
       "      <th>zeroth</th>\n",
       "      <th>zip</th>\n",
       "      <th>ã¼nã¼yorum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00000  000000  000196  000282  000284  000360  000469  000750  000886  \\\n",
       "0    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4    0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   000991  ...  yticks   yã   yä  zaman  zdã¼m  zero  zerodivisionerror  \\\n",
       "0     0.0  ...     0.0  0.0  0.0    0.0    0.0   0.0                0.0   \n",
       "1     0.0  ...     0.0  0.0  0.0    0.0    0.0   0.0                0.0   \n",
       "2     0.0  ...     0.0  0.0  0.0    0.0    0.0   0.0                0.0   \n",
       "3     0.0  ...     0.0  0.0  0.0    0.0    0.0   0.0                0.0   \n",
       "4     0.0  ...     0.0  0.0  0.0    0.0    0.0   0.0                0.0   \n",
       "\n",
       "   zeroth  zip  ã¼nã¼yorum  \n",
       "0     0.0  0.0         0.0  \n",
       "1     0.0  0.0         0.0  \n",
       "2     0.0  0.0         0.0  \n",
       "3     0.0  0.0         0.0  \n",
       "4     0.0  0.0         0.0  \n",
       "\n",
       "[5 rows x 5400 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code2prompts_tf_idf[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 5400)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code2prompts_tf_idf[\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2cosine = dict()\n",
    "for code, user_prompts_tf_idf in code2prompts_tf_idf.items():\n",
    "    code2cosine[code] = pd.DataFrame(cosine_similarity(questions_TF_IDF,user_prompts_tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Features and Correlations (10 pts)\\n\\n* Correlations of features with health (4 points)\\nCalculate the correlations for all features in dataset. Highlight any strong correlations with the target variable. Plot your results in a heatmap.\\n\\n* Feature Selection (3 points)\\nSelect a subset of features that are likely strong predictors, justifying your choices based on the computed correlations.\\n\\n* Hypothetical Driver Features (3 points)\\nPropose two hypothetical features that could enhance the model's predictive accuracy for Y, explaining how they might be derived and their expected impact. Show the resulting correlations with target variable.\\n\\n* __Note:__ You get can get help from GPT.\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>Q_0</th>\n",
       "      <th>Q_1</th>\n",
       "      <th>Q_2</th>\n",
       "      <th>Q_3</th>\n",
       "      <th>Q_4</th>\n",
       "      <th>Q_5</th>\n",
       "      <th>Q_6</th>\n",
       "      <th>Q_7</th>\n",
       "      <th>Q_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0031c86e-81f4-4eef-9e0e-28037abf9883</td>\n",
       "      <td>0.153541</td>\n",
       "      <td>0.294879</td>\n",
       "      <td>0.522169</td>\n",
       "      <td>0.352243</td>\n",
       "      <td>0.460606</td>\n",
       "      <td>0.553876</td>\n",
       "      <td>0.167250</td>\n",
       "      <td>0.182938</td>\n",
       "      <td>0.195440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0225686d-b825-4cac-8691-3a3a5343df2b</td>\n",
       "      <td>0.192013</td>\n",
       "      <td>0.795607</td>\n",
       "      <td>0.772184</td>\n",
       "      <td>0.882656</td>\n",
       "      <td>0.607114</td>\n",
       "      <td>0.987511</td>\n",
       "      <td>0.892586</td>\n",
       "      <td>0.570741</td>\n",
       "      <td>0.543866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>041f950b-c013-409a-a642-cffff60b9d4b</td>\n",
       "      <td>0.258306</td>\n",
       "      <td>0.295923</td>\n",
       "      <td>0.624824</td>\n",
       "      <td>0.351872</td>\n",
       "      <td>0.643038</td>\n",
       "      <td>0.454314</td>\n",
       "      <td>0.540269</td>\n",
       "      <td>0.546506</td>\n",
       "      <td>0.325793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04f91058-d0f8-4324-83b2-19c671f433dc</td>\n",
       "      <td>0.145965</td>\n",
       "      <td>0.117841</td>\n",
       "      <td>0.267346</td>\n",
       "      <td>0.316809</td>\n",
       "      <td>0.333889</td>\n",
       "      <td>0.309084</td>\n",
       "      <td>0.192434</td>\n",
       "      <td>0.261892</td>\n",
       "      <td>0.407106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>089eb66d-4c3a-4f58-b98f-a3774a2efb34</td>\n",
       "      <td>0.344182</td>\n",
       "      <td>0.575528</td>\n",
       "      <td>0.782883</td>\n",
       "      <td>0.624833</td>\n",
       "      <td>0.724872</td>\n",
       "      <td>0.872171</td>\n",
       "      <td>0.684797</td>\n",
       "      <td>0.945305</td>\n",
       "      <td>0.511769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>f24219d6-07f0-4baf-80ac-18475dc5b66f</td>\n",
       "      <td>0.187393</td>\n",
       "      <td>0.209889</td>\n",
       "      <td>0.358875</td>\n",
       "      <td>0.211246</td>\n",
       "      <td>0.149281</td>\n",
       "      <td>0.507128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862585</td>\n",
       "      <td>0.739634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>f2f18684-4a16-4c05-a2d1-c0f96d1de869</td>\n",
       "      <td>0.144760</td>\n",
       "      <td>0.139495</td>\n",
       "      <td>0.332267</td>\n",
       "      <td>0.950344</td>\n",
       "      <td>0.775698</td>\n",
       "      <td>0.828355</td>\n",
       "      <td>0.374145</td>\n",
       "      <td>0.905688</td>\n",
       "      <td>0.553341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>f852596d-fdca-45aa-9050-d4f76ce6a53c</td>\n",
       "      <td>0.211008</td>\n",
       "      <td>0.908697</td>\n",
       "      <td>0.977725</td>\n",
       "      <td>0.924311</td>\n",
       "      <td>0.888609</td>\n",
       "      <td>0.921306</td>\n",
       "      <td>0.779558</td>\n",
       "      <td>0.767328</td>\n",
       "      <td>0.499508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>f8ec3336-fd48-4654-ad98-62ccfb96d096</td>\n",
       "      <td>0.173215</td>\n",
       "      <td>0.967321</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.703987</td>\n",
       "      <td>0.942662</td>\n",
       "      <td>0.907417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.782938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>fb8de815-224c-4d06-9fd4-7156d1a9920d</td>\n",
       "      <td>0.170293</td>\n",
       "      <td>0.712409</td>\n",
       "      <td>0.835236</td>\n",
       "      <td>0.760650</td>\n",
       "      <td>0.895230</td>\n",
       "      <td>0.943543</td>\n",
       "      <td>0.944887</td>\n",
       "      <td>0.844004</td>\n",
       "      <td>0.729632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     code       Q_0       Q_1       Q_2  \\\n",
       "0    0031c86e-81f4-4eef-9e0e-28037abf9883  0.153541  0.294879  0.522169   \n",
       "1    0225686d-b825-4cac-8691-3a3a5343df2b  0.192013  0.795607  0.772184   \n",
       "2    041f950b-c013-409a-a642-cffff60b9d4b  0.258306  0.295923  0.624824   \n",
       "3    04f91058-d0f8-4324-83b2-19c671f433dc  0.145965  0.117841  0.267346   \n",
       "4    089eb66d-4c3a-4f58-b98f-a3774a2efb34  0.344182  0.575528  0.782883   \n",
       "..                                    ...       ...       ...       ...   \n",
       "118  f24219d6-07f0-4baf-80ac-18475dc5b66f  0.187393  0.209889  0.358875   \n",
       "119  f2f18684-4a16-4c05-a2d1-c0f96d1de869  0.144760  0.139495  0.332267   \n",
       "120  f852596d-fdca-45aa-9050-d4f76ce6a53c  0.211008  0.908697  0.977725   \n",
       "121  f8ec3336-fd48-4654-ad98-62ccfb96d096  0.173215  0.967321  1.000000   \n",
       "122  fb8de815-224c-4d06-9fd4-7156d1a9920d  0.170293  0.712409  0.835236   \n",
       "\n",
       "          Q_3       Q_4       Q_5       Q_6       Q_7       Q_8  \n",
       "0    0.352243  0.460606  0.553876  0.167250  0.182938  0.195440  \n",
       "1    0.882656  0.607114  0.987511  0.892586  0.570741  0.543866  \n",
       "2    0.351872  0.643038  0.454314  0.540269  0.546506  0.325793  \n",
       "3    0.316809  0.333889  0.309084  0.192434  0.261892  0.407106  \n",
       "4    0.624833  0.724872  0.872171  0.684797  0.945305  0.511769  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "118  0.211246  0.149281  0.507128  1.000000  0.862585  0.739634  \n",
       "119  0.950344  0.775698  0.828355  0.374145  0.905688  0.553341  \n",
       "120  0.924311  0.888609  0.921306  0.779558  0.767328  0.499508  \n",
       "121  0.703987  0.942662  0.907417  1.000000  1.000000  0.782938  \n",
       "122  0.760650  0.895230  0.943543  0.944887  0.844004  0.729632  \n",
       "\n",
       "[123 rows x 10 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code2questionmapping = dict()\n",
    "for code, cosine_scores in code2cosine.items():\n",
    "    code2questionmapping[code] = code2cosine[code].max(axis=1).tolist()\n",
    "\n",
    "\n",
    "question_mapping_scores = pd.DataFrame(code2questionmapping).T\n",
    "question_mapping_scores.reset_index(inplace=True)\n",
    "question_mapping_scores.rename(columns={i: f\"Q_{i}\" for i in range(len(questions))}, inplace=True)\n",
    "question_mapping_scores.rename(columns={\"index\" : \"code\"}, inplace=True)\n",
    "\n",
    "question_mapping_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'df': 87, 'import': 33, 'plt': 32, 'as': 29, 'pd': 28, 'file_path': 28, 'print': 15, '1': 15, 'correlations': 15, 'pandas': 14, 'cs412_hw1_dataset': 14, 'csv': 14, 'read_csv': 14, 'map': 14, 'sns': 14, 'diet': 13, '2': 13, 'health_metrics': 13, 'health_metrics_encoded': 13, 'health': 12, 'metrics': 12, 'health_metrics_map': 12, 'true': 11, 'the': 10, 'corr': 10, 'column': 9, '0': 9, '3': 9, '10': 9, 'heatmap': 9, 'x': 8, 'drop': 8, 'y': 8, 'of': 7, 'columns': 7, 'seaborn': 7, 'matplotlib': 7, 'pyplot': 7, 'correlation': 7, 'figure': 7, 'figsize': 7, '6': 7, 'title': 7, 'show': 7, 'variable': 6, 'healthy': 6, 'overweight': 6, 'underweight': 6, 'features': 6, '2f': 6, 'you': 6, 'can': 6, 'seasonality': 6, 'for': 5, 'from': 5, 'sklearn': 5, 'train_test_split': 5, 'mode': 5, 'fillna': 5, 'inplace': 5, 'species': 5, 'target': 5, 'random_state': 5, 'with': 5, 'annot': 5, 'cmap': 5, 'coolwarm': 5, 'fmt': 5, 'grid_search': 5, 'shape': 4, 'dataset': 4, 'sex': 4, 'island': 4, 'life': 4, 'stage': 4, '42': 4, 'numerical_columns': 4, 'correlation_caloric_intake': 4, 'correlation_seasonality': 4, 'daily': 4, 'caloric': 4, 'intake': 4, 'if': 4, 'names': 3, 'in': 3, '5': 3, 'model_selection': 3, 'fish': 3, 'krill': 3, 'squid': 3, 'parental': 3, 'excluding': 3, 'number': 3, 'to_frame': 3, 'encoded': 3, 'correlation_threshold': 3, 'your': 3, 'it': 3, 'access': 3, 'param_grid': 3, 'depth': 3, 'head': 2, 'rows': 2, 'variable_names': 2, 'name': 2, 'most_common_sex': 2, 'most_common_island': 2, 'most_common_diet': 2, 'most_common_life_stage': 2, 'most_common_health_metrics': 2, 'sex_map': 2, 'island_map': 2, 'diet_map': 2, 'life_stage_map': 2, 'x_train': 2, 'x_test': 2, 'y_train': 2, 'y_test': 2, 'test_size': 2, 'and': 2, 'labelencoder': 2, 'label_encoder': 2, 'species_encoded': 2, 'select_dtypes': 2, 'include': 2, 'corrwith': 2, 'sort_values': 2, 'ascending': 2, 'false': 2, 'this': 2, 'based': 2, 'on': 2, 'selected_features': 2, 'caloric_intake': 2, '500': 2, '300': 2, '400': 2, '600': 2, 'body': 2, 'mass': 2, 'g': 2, 'year': 2, 'winter': 2, 'spring': 2, 'summer': 2, 'autumn': 2, 'barplot': 2, 'new': 2, 'ylabel': 2, 'coefficient': 2, 'ylim': 2, 'not': 2, 'is': 2, 'handle': 2, 'a': 2, 'or': 2, 'exists': 2, 'diet_column': 2, 'diet_caloric_map': 2, 'seasonality_map': 2, 'gridsearchcv': 2, 'tree': 2, 'decisiontreeclassifier': 2, 'max_depth': 2, 'none': 2, 'min_samples_split': 2, 'samples': 2, 'split': 2, 'clf': 2, 'accuracy': 2, 'best_max_depth': 2, 'best_params_': 2, 'best_min_samples_split': 2, 'best_accuracy': 2, 'f': 2, 'best': 2, 'nsummary': 1, 'info': 1, 'nfirst': 1, 'female': 1, 'male': 1, 'biscoe': 1, 'dream': 1, 'torgensen': 1, '4': 1, 'chick': 1, 'juvenile': 1, 'adult': 1, 'sample': 1, 'frac': 1, 'reset_index': 1, 'shuffle': 1, 'reproducibility': 1, 'remove': 1, 'self': 1, 'select': 1, 'preprocessing': 1, 'fit_transform': 1, 'adjust': 1, 'threshold': 1, 'requirements': 1, 'selected': 1, '2017': 1, '2018': 1, '2019': 1, '2020': 1, 'dataframe': 1, 'found': 1, 'needs': 1, 'example': 1, 'different': 1, 'similar': 1, 'purpose': 1, 'available': 1, 'perform': 1, 'other': 1, 'data': 1, 'handling': 1, 'tasks': 1, 'missing': 1, 'please': 1, 'check': 1, 'structure': 1, 'situation': 1, 'accordingly': 1, 'else': 1, 'now': 1, 'use': 1, 'calculations': 1, '2007': 1, '2008': 1, '2009': 1, '2010': 1, '20': 1, '30': 1, 'maximum': 1, 'unlimited': 1, 'minimum': 1, 'required': 1, 'to': 1, 'an': 1, 'internal': 1, 'node': 1, 'estimator': 1, 'cv': 1, 'scoring': 1, 'fit': 1, 'best_score_': 1, 'hyperparameters': 1, 'max': 1, 'min': 1, 'cross': 1, 'validation': 1})\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to hold word counts for each file code\n",
    "word_counts = {}\n",
    "\n",
    "for file_code, conversations in code2convos.items():\n",
    "    text_counter = Counter()\n",
    "    code_counter = Counter()\n",
    "\n",
    "    for convo in conversations:\n",
    "        # Tokenize and count words in the text part\n",
    "        if 'text' in convo:\n",
    "            text_counter.update(tokenize(convo['text']))\n",
    "        \n",
    "        # Tokenize and count words in the code part\n",
    "        if 'code' in convo:\n",
    "            code_counter.update(tokenize(convo['code']))\n",
    "\n",
    "    word_counts[file_code] = {'text': text_counter, 'code': code_counter}\n",
    "\n",
    "# Example: Display word counts for a specific file code\n",
    "print(word_counts['0031c86e-81f4-4eef-9e0e-28037abf9883'][\"code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Number of prompts that a uers asked\n",
    "- Number of complaints that a user makes e.g \"the code gives this error!\"\n",
    "- User prompts average number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139235c7-736c-4237-92f0-92e8c116832c\n",
      "668ad17e-0240-49f7-b5a7-d22e502554c6\n",
      "b0640e51-6879-40cb-a4f5-329f952ef99d\n",
      "da6b70d5-29f6-491a-ad46-037c77067128\n"
     ]
    }
   ],
   "source": [
    "code2features = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "keywords2search = [\"error\", \"no\", \"thank\", \"next\", \"Entropy\"]\n",
    "keywords2search = [k.lower() for k in keywords2search]\n",
    "total_code_length = 0  # Initialize a variable to track total code length\n",
    "\n",
    "\n",
    "for code, convs in code2convos.items():\n",
    "    if len(convs) == 0:\n",
    "        print(code)\n",
    "        continue\n",
    "    for c in convs:\n",
    "        text = c[\"text\"].lower()\n",
    "        if c[\"role\"] == \"user\":\n",
    "            # User Prompts\n",
    "\n",
    "            # count the user prompts\n",
    "            code2features[code][\"#user_prompts\"] += 1\n",
    "            \n",
    "            # count the keywords\n",
    "            for kw in keywords2search:\n",
    "                code2features[code][f\"#{kw}\"] +=  len(re.findall(rf\"\\b{kw}\\b\", text))\n",
    "\n",
    "            code2features[code][\"prompt_avg_chars\"] += len(text)\n",
    "        else:\n",
    "            # ChatGPT Responses\n",
    "            code2features[code][\"response_avg_chars\"] += len(text)\n",
    "\n",
    "        \n",
    "        code2features[code][\"prompt_avg_chars\"] /= code2features[code][\"#user_prompts\"]   \n",
    "        code2features[code][\"response_avg_chars\"] /= code2features[code][\"#user_prompts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# count average code length in terms of number of chars in the code part of a response of ChatGPT\n",
    "\n",
    "total_sum = 0\n",
    "num = 0\n",
    "sum = 0\n",
    "for code, convs in code2convos.items():\n",
    "    for c in convs:\n",
    "        if c[\"role\"] == \"assistant\":\n",
    "            num += 1\n",
    "            sum = 0\n",
    "            for line in c[\"code\"]:\n",
    "                sum += len(line)\n",
    "            total_sum += sum\n",
    "    code2features[code][\"avg_code_length\"] = (total_sum/num)\n",
    "    \n",
    "    ''''\n",
    "    if code2features[code][\"response_avg_chars\"] > 0:\n",
    "        code2features[code][\"code_response_ratio\"] = code2features[code][\"avg_code_length\"] / code2features[code][\"response_avg_chars\"]\n",
    "    else: \n",
    "        print(code2features[code][\"response_avg_chars\"])\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load the English NLP model once\n",
    "\n",
    "def is_imperative(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Check for common characteristics of imperative sentences:\n",
    "    if len(doc) > 0:\n",
    "        first_token = doc[0]\n",
    "\n",
    "        # 1. Verb in imperative form at the beginning\n",
    "        if first_token.pos_ == \"VERB\" and first_token.tag_ in (\"VB\", \"VBP\"):\n",
    "\n",
    "            # 2. No subject or explicit subject\n",
    "            if not first_token.dep_ or first_token.dep_ not in (\"nsubj\", \"nsubjpass\"):\n",
    "\n",
    "                # 3. Handle exceptions and potential false positives:\n",
    "                if not (first_token.text.lower() in (\"let's\", \"let's\") and doc[1].dep_ == \"obj\"):  # Exclude \"Let's\" suggestions\n",
    "                    return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load the English NLP model once for efficiency\n",
    "\n",
    "\n",
    "for code, convos in code2convos.items():\n",
    "    imperative_sentence_count = 0\n",
    "\n",
    "    for conv in convos:\n",
    "        if conv[\"role\"] == \"user\":\n",
    "            text = conv[\"text\"]\n",
    "        \n",
    "\n",
    "            # Check for imperative sentences\n",
    "            sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\", text)  # Split text into sentences\n",
    "            for sentence in sentences:\n",
    "                if is_imperative(sentence):\n",
    "                    imperative_sentence_count += 1\n",
    "\n",
    "    code2features[code][\"imperative_sentence_count\"] =  imperative_sentence_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#user_prompts</th>\n",
       "      <th>#error</th>\n",
       "      <th>#no</th>\n",
       "      <th>#thank</th>\n",
       "      <th>#next</th>\n",
       "      <th>#entropy</th>\n",
       "      <th>prompt_avg_chars</th>\n",
       "      <th>response_avg_chars</th>\n",
       "      <th>avg_code_length</th>\n",
       "      <th>imperative_sentence_count</th>\n",
       "      <th>code_response_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0031c86e-81f4-4eef-9e0e-28037abf9883</th>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.205748</td>\n",
       "      <td>113.703886</td>\n",
       "      <td>648.285714</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.701526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0225686d-b825-4cac-8691-3a3a5343df2b</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.304163</td>\n",
       "      <td>56.984332</td>\n",
       "      <td>525.843750</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.227865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>041f950b-c013-409a-a642-cffff60b9d4b</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.262324</td>\n",
       "      <td>15.134767</td>\n",
       "      <td>477.414634</td>\n",
       "      <td>14.0</td>\n",
       "      <td>31.544234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04f91058-d0f8-4324-83b2-19c671f433dc</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.123346</td>\n",
       "      <td>27.642008</td>\n",
       "      <td>488.918033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089eb66d-4c3a-4f58-b98f-a3774a2efb34</th>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>18.850776</td>\n",
       "      <td>483.979592</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.674253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>090d6217-5d69-4929-a342-19abab78324f</th>\n",
       "      <td>73.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.006570</td>\n",
       "      <td>3.041949</td>\n",
       "      <td>525.900000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>172.882600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0c95c563-f1e1-4a35-844c-2e8ccdd1b161</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.380935</td>\n",
       "      <td>92.922751</td>\n",
       "      <td>515.540426</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.548054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0ddfae9c-0dbd-4fbe-9e68-c3e0cb73b8fc</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.379206</td>\n",
       "      <td>44.147885</td>\n",
       "      <td>503.378049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.402088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0e466794-f7f8-4cc2-b07e-070b13a0b5e1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.006671</td>\n",
       "      <td>9.034062</td>\n",
       "      <td>468.486301</td>\n",
       "      <td>18.0</td>\n",
       "      <td>51.857770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0f0c953a-a472-47c1-809b-9fc14dba9091</th>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302888</td>\n",
       "      <td>31.829341</td>\n",
       "      <td>462.423948</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.528229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029802d-1057-4e3e-b827-e8a9c2ded3b9</th>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.017075</td>\n",
       "      <td>6.311168</td>\n",
       "      <td>468.268698</td>\n",
       "      <td>33.0</td>\n",
       "      <td>74.196839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038cb22-8ad2-4030-b44a-59f10352e91c</th>\n",
       "      <td>62.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.014051</td>\n",
       "      <td>8.986707</td>\n",
       "      <td>492.737589</td>\n",
       "      <td>34.0</td>\n",
       "      <td>54.829602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106ffe99-c787-4d09-9076-4ba411eb68b1</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.019503</td>\n",
       "      <td>7.058539</td>\n",
       "      <td>485.513393</td>\n",
       "      <td>15.0</td>\n",
       "      <td>68.783838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14ce054d-4619-4685-ba9b-4b5cd6f81b2d</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.975743</td>\n",
       "      <td>75.012074</td>\n",
       "      <td>480.366812</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152a7787-ecd1-448f-a98e-8af0826d8215</th>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.113583</td>\n",
       "      <td>53.614062</td>\n",
       "      <td>477.844262</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17bd62e7-6792-4399-a573-07456a73901b</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.048272</td>\n",
       "      <td>12.765734</td>\n",
       "      <td>478.542857</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1b54e38b-3b1d-425e-835a-d1e0fb2694fc</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.609788</td>\n",
       "      <td>79.007018</td>\n",
       "      <td>475.145488</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223ae726-cb25-49df-a125-c4af3519c8e8</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.217072</td>\n",
       "      <td>44.564871</td>\n",
       "      <td>471.282648</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22bb7162-3399-464a-b30b-cf1fc3210b4e</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.718688</td>\n",
       "      <td>29.477678</td>\n",
       "      <td>468.587931</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446216c-c557-4ee8-b470-7e2ae3c88968</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.198889</td>\n",
       "      <td>545.025972</td>\n",
       "      <td>467.822222</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      #user_prompts  #error  #no  #thank  \\\n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883           14.0     3.0  1.0     0.0   \n",
       "0225686d-b825-4cac-8691-3a3a5343df2b           18.0     0.0  0.0     0.0   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b            9.0     3.0  0.0     1.0   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc           20.0     1.0  1.0     0.0   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34           86.0     1.0  5.0     0.0   \n",
       "090d6217-5d69-4929-a342-19abab78324f           73.0    13.0  0.0     0.0   \n",
       "0c95c563-f1e1-4a35-844c-2e8ccdd1b161           15.0     0.0  0.0     0.0   \n",
       "0ddfae9c-0dbd-4fbe-9e68-c3e0cb73b8fc           11.0     0.0  0.0     0.0   \n",
       "0e466794-f7f8-4cc2-b07e-070b13a0b5e1           46.0     6.0  0.0     0.0   \n",
       "0f0c953a-a472-47c1-809b-9fc14dba9091           17.0     3.0  0.0     0.0   \n",
       "1029802d-1057-4e3e-b827-e8a9c2ded3b9           52.0     4.0  1.0     0.0   \n",
       "1038cb22-8ad2-4030-b44a-59f10352e91c           62.0     4.0  1.0     0.0   \n",
       "106ffe99-c787-4d09-9076-4ba411eb68b1           25.0     0.0  0.0     0.0   \n",
       "14ce054d-4619-4685-ba9b-4b5cd6f81b2d           10.0     1.0  0.0     0.0   \n",
       "152a7787-ecd1-448f-a98e-8af0826d8215           30.0     2.0  0.0     0.0   \n",
       "17bd62e7-6792-4399-a573-07456a73901b           37.0     1.0  1.0     0.0   \n",
       "1b54e38b-3b1d-425e-835a-d1e0fb2694fc           18.0     1.0  1.0     0.0   \n",
       "223ae726-cb25-49df-a125-c4af3519c8e8           16.0     0.0  0.0     0.0   \n",
       "22bb7162-3399-464a-b30b-cf1fc3210b4e           21.0     0.0  1.0     1.0   \n",
       "2446216c-c557-4ee8-b470-7e2ae3c88968            5.0     0.0  0.0     0.0   \n",
       "\n",
       "                                      #next  #entropy  prompt_avg_chars  \\\n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883    0.0       0.0          2.205748   \n",
       "0225686d-b825-4cac-8691-3a3a5343df2b    0.0       3.0          0.304163   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b    0.0       3.0          0.262324   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc    0.0       3.0          0.123346   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34    0.0      26.0          0.017986   \n",
       "090d6217-5d69-4929-a342-19abab78324f    0.0      19.0          0.006570   \n",
       "0c95c563-f1e1-4a35-844c-2e8ccdd1b161    0.0       3.0          0.380935   \n",
       "0ddfae9c-0dbd-4fbe-9e68-c3e0cb73b8fc    0.0       1.0          0.379206   \n",
       "0e466794-f7f8-4cc2-b07e-070b13a0b5e1    0.0       3.0          0.006671   \n",
       "0f0c953a-a472-47c1-809b-9fc14dba9091    1.0       0.0          0.302888   \n",
       "1029802d-1057-4e3e-b827-e8a9c2ded3b9    0.0       9.0          0.017075   \n",
       "1038cb22-8ad2-4030-b44a-59f10352e91c    0.0       2.0          0.014051   \n",
       "106ffe99-c787-4d09-9076-4ba411eb68b1    0.0       2.0          0.019503   \n",
       "14ce054d-4619-4685-ba9b-4b5cd6f81b2d    0.0       2.0          7.975743   \n",
       "152a7787-ecd1-448f-a98e-8af0826d8215    0.0       3.0          0.113583   \n",
       "17bd62e7-6792-4399-a573-07456a73901b    0.0       3.0          0.048272   \n",
       "1b54e38b-3b1d-425e-835a-d1e0fb2694fc    0.0       2.0          0.609788   \n",
       "223ae726-cb25-49df-a125-c4af3519c8e8    0.0       1.0          0.217072   \n",
       "22bb7162-3399-464a-b30b-cf1fc3210b4e    0.0       3.0          0.718688   \n",
       "2446216c-c557-4ee8-b470-7e2ae3c88968    0.0       0.0         14.198889   \n",
       "\n",
       "                                      response_avg_chars  avg_code_length  \\\n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883          113.703886       648.285714   \n",
       "0225686d-b825-4cac-8691-3a3a5343df2b           56.984332       525.843750   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b           15.134767       477.414634   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc           27.642008       488.918033   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34           18.850776       483.979592   \n",
       "090d6217-5d69-4929-a342-19abab78324f            3.041949       525.900000   \n",
       "0c95c563-f1e1-4a35-844c-2e8ccdd1b161           92.922751       515.540426   \n",
       "0ddfae9c-0dbd-4fbe-9e68-c3e0cb73b8fc           44.147885       503.378049   \n",
       "0e466794-f7f8-4cc2-b07e-070b13a0b5e1            9.034062       468.486301   \n",
       "0f0c953a-a472-47c1-809b-9fc14dba9091           31.829341       462.423948   \n",
       "1029802d-1057-4e3e-b827-e8a9c2ded3b9            6.311168       468.268698   \n",
       "1038cb22-8ad2-4030-b44a-59f10352e91c            8.986707       492.737589   \n",
       "106ffe99-c787-4d09-9076-4ba411eb68b1            7.058539       485.513393   \n",
       "14ce054d-4619-4685-ba9b-4b5cd6f81b2d           75.012074       480.366812   \n",
       "152a7787-ecd1-448f-a98e-8af0826d8215           53.614062       477.844262   \n",
       "17bd62e7-6792-4399-a573-07456a73901b           12.765734       478.542857   \n",
       "1b54e38b-3b1d-425e-835a-d1e0fb2694fc           79.007018       475.145488   \n",
       "223ae726-cb25-49df-a125-c4af3519c8e8           44.564871       471.282648   \n",
       "22bb7162-3399-464a-b30b-cf1fc3210b4e           29.477678       468.587931   \n",
       "2446216c-c557-4ee8-b470-7e2ae3c88968          545.025972       467.822222   \n",
       "\n",
       "                                      imperative_sentence_count  \\\n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883                       17.0   \n",
       "0225686d-b825-4cac-8691-3a3a5343df2b                       20.0   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b                       14.0   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc                        1.0   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34                       26.0   \n",
       "090d6217-5d69-4929-a342-19abab78324f                       45.0   \n",
       "0c95c563-f1e1-4a35-844c-2e8ccdd1b161                        6.0   \n",
       "0ddfae9c-0dbd-4fbe-9e68-c3e0cb73b8fc                        0.0   \n",
       "0e466794-f7f8-4cc2-b07e-070b13a0b5e1                       18.0   \n",
       "0f0c953a-a472-47c1-809b-9fc14dba9091                        7.0   \n",
       "1029802d-1057-4e3e-b827-e8a9c2ded3b9                       33.0   \n",
       "1038cb22-8ad2-4030-b44a-59f10352e91c                       34.0   \n",
       "106ffe99-c787-4d09-9076-4ba411eb68b1                       15.0   \n",
       "14ce054d-4619-4685-ba9b-4b5cd6f81b2d                        5.0   \n",
       "152a7787-ecd1-448f-a98e-8af0826d8215                       12.0   \n",
       "17bd62e7-6792-4399-a573-07456a73901b                       10.0   \n",
       "1b54e38b-3b1d-425e-835a-d1e0fb2694fc                        7.0   \n",
       "223ae726-cb25-49df-a125-c4af3519c8e8                       13.0   \n",
       "22bb7162-3399-464a-b30b-cf1fc3210b4e                       11.0   \n",
       "2446216c-c557-4ee8-b470-7e2ae3c88968                        4.0   \n",
       "\n",
       "                                      code_response_ratio  \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883             5.701526  \n",
       "0225686d-b825-4cac-8691-3a3a5343df2b             9.227865  \n",
       "041f950b-c013-409a-a642-cffff60b9d4b            31.544234  \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc            17.687500  \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34            25.674253  \n",
       "090d6217-5d69-4929-a342-19abab78324f           172.882600  \n",
       "0c95c563-f1e1-4a35-844c-2e8ccdd1b161             5.548054  \n",
       "0ddfae9c-0dbd-4fbe-9e68-c3e0cb73b8fc            11.402088  \n",
       "0e466794-f7f8-4cc2-b07e-070b13a0b5e1            51.857770  \n",
       "0f0c953a-a472-47c1-809b-9fc14dba9091            14.528229  \n",
       "1029802d-1057-4e3e-b827-e8a9c2ded3b9            74.196839  \n",
       "1038cb22-8ad2-4030-b44a-59f10352e91c            54.829602  \n",
       "106ffe99-c787-4d09-9076-4ba411eb68b1            68.783838  \n",
       "14ce054d-4619-4685-ba9b-4b5cd6f81b2d                  NaN  \n",
       "152a7787-ecd1-448f-a98e-8af0826d8215                  NaN  \n",
       "17bd62e7-6792-4399-a573-07456a73901b                  NaN  \n",
       "1b54e38b-3b1d-425e-835a-d1e0fb2694fc                  NaN  \n",
       "223ae726-cb25-49df-a125-c4af3519c8e8                  NaN  \n",
       "22bb7162-3399-464a-b30b-cf1fc3210b4e                  NaN  \n",
       "2446216c-c557-4ee8-b470-7e2ae3c88968                  NaN  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(code2features).T\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6a2003ad-a05a-41c9-9d48-e98491a90499</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04f91058-d0f8-4324-83b2-19c671f433dc</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81fdeb2a-e7e5-4a05-8058-d31ea579b0d9</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6a903495-c5be-4263-b4dd-75e2bbc30434</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6d5742c1-77c4-429c-8f6e-ef1262ca5557</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   code  grade\n",
       "0  6a2003ad-a05a-41c9-9d48-e98491a90499   90.0\n",
       "1  04f91058-d0f8-4324-83b2-19c671f433dc   97.0\n",
       "2  81fdeb2a-e7e5-4a05-8058-d31ea579b0d9   94.0\n",
       "3  6a903495-c5be-4263-b4dd-75e2bbc30434   97.0\n",
       "4  6d5742c1-77c4-429c-8f6e-ef1262ca5557   93.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # reading the scores\n",
    "# scores = pd.read_csv(\"data/scores.csv\", sep=\",\")\n",
    "scores[\"code\"] = scores[\"code\"].apply(lambda x: x.strip())\n",
    "\n",
    "# selecting the columns we need and we care\n",
    "scores = scores[[\"code\", \"grade\"]]\n",
    "\n",
    "# show some examples\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's check grades distribution\n",
    "\n",
    "plt.title('Histogram Grades')\n",
    "plt.hist(scores[\"grade\"], rwidth=.8, bins=np.arange(min(scores[\"grade\"]), max(scores[\"grade\"])+2) - 0.5)\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>#user_prompts</th>\n",
       "      <th>#error</th>\n",
       "      <th>#no</th>\n",
       "      <th>#thank</th>\n",
       "      <th>#next</th>\n",
       "      <th>#entropy</th>\n",
       "      <th>prompt_avg_chars</th>\n",
       "      <th>response_avg_chars</th>\n",
       "      <th>avg_code_length</th>\n",
       "      <th>imperative_sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0031c86e-81f4-4eef-9e0e-28037abf9883</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.205748</td>\n",
       "      <td>113.703886</td>\n",
       "      <td>648.285714</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0225686d-b825-4cac-8691-3a3a5343df2b</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.304163</td>\n",
       "      <td>56.984332</td>\n",
       "      <td>525.843750</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>041f950b-c013-409a-a642-cffff60b9d4b</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.262324</td>\n",
       "      <td>15.134767</td>\n",
       "      <td>477.414634</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04f91058-d0f8-4324-83b2-19c671f433dc</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.123346</td>\n",
       "      <td>27.642008</td>\n",
       "      <td>488.918033</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>089eb66d-4c3a-4f58-b98f-a3774a2efb34</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>18.850776</td>\n",
       "      <td>483.979592</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   code  #user_prompts  #error  #no  #thank  \\\n",
       "0  0031c86e-81f4-4eef-9e0e-28037abf9883           14.0     3.0  1.0     0.0   \n",
       "1  0225686d-b825-4cac-8691-3a3a5343df2b           18.0     0.0  0.0     0.0   \n",
       "2  041f950b-c013-409a-a642-cffff60b9d4b            9.0     3.0  0.0     1.0   \n",
       "3  04f91058-d0f8-4324-83b2-19c671f433dc           20.0     1.0  1.0     0.0   \n",
       "4  089eb66d-4c3a-4f58-b98f-a3774a2efb34           86.0     1.0  5.0     0.0   \n",
       "\n",
       "   #next  #entropy  prompt_avg_chars  response_avg_chars  avg_code_length  \\\n",
       "0    0.0       0.0          2.205748          113.703886       648.285714   \n",
       "1    0.0       3.0          0.304163           56.984332       525.843750   \n",
       "2    0.0       3.0          0.262324           15.134767       477.414634   \n",
       "3    0.0       3.0          0.123346           27.642008       488.918033   \n",
       "4    0.0      26.0          0.017986           18.850776       483.979592   \n",
       "\n",
       "   imperative_sentence_count  \n",
       "0                       17.0  \n",
       "1                       20.0  \n",
       "2                       14.0  \n",
       "3                        1.0  \n",
       "4                       26.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(inplace=True, drop=False)\n",
    "df.rename(columns={\"index\": \"code\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17944\\1353847570.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_mapping_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"code\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0mindicator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m ) -> DataFrame:\n\u001b[1;32m--> 110\u001b[1;33m     op = _MergeOperation(\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    699\u001b[0m         (\n\u001b[0;32m    700\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[1;31m# to avoid incompatible dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1179\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1180\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                         \u001b[1;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[assignment]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m             )\n\u001b[0;32m   1849\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'code'"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, question_mapping_scores, on=\"code\", how=\"left\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging scores with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17944\\2765275381.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtemp_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'code'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"code\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"first\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0mindicator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m ) -> DataFrame:\n\u001b[1;32m--> 110\u001b[1;33m     op = _MergeOperation(\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    699\u001b[0m         (\n\u001b[0;32m    700\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[1;31m# to avoid incompatible dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1179\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1180\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                         \u001b[1;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[assignment]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m             )\n\u001b[0;32m   1849\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'code'"
     ]
    }
   ],
   "source": [
    "temp_df = pd.merge(df, scores, on='code', how=\"left\")\n",
    "temp_df.dropna(inplace=True)\n",
    "temp_df.drop_duplicates(\"code\",inplace=True, keep=\"first\")\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Vectorize code\\nvectorizer = CountVectorizer(tokenizer=tokenize_code)  # Use custom tokenizer\\ncode_count_matrix = vectorizer.fit_transform(code_data)\\n\\nquestion_vector = vectorizer.transform([question])\\nsimilarity = cosine_similarity(question_vector, code_count_matrix)\\n\\nprint(\"Similarity with code:\", similarity)\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code_data = temp_df[\"code\"].tolist()\n",
    "'''\n",
    "# Vectorize code\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_code)  # Use custom tokenizer\n",
    "code_count_matrix = vectorizer.fit_transform(code_data)\n",
    "\n",
    "question_vector = vectorizer.transform([question])\n",
    "similarity = cosine_similarity(question_vector, code_count_matrix)\n",
    "\n",
    "print(\"Similarity with code:\", similarity)\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122, 19) (122,)\n"
     ]
    }
   ],
   "source": [
    "X = temp_df[temp_df.columns[1:-1]].to_numpy()\n",
    "y = temp_df[\"grade\"].to_numpy()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 97\n",
      "Test set size: 25\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Train set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting and Analyzing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=10, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=10, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=10, random_state=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = DecisionTreeRegressor(random_state=0,criterion='squared_error', max_depth=10)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 0 has MSE 163.42034222552866\n",
      "Node 1 has MSE 126.39409722222263\n",
      "Node 2 has MSE 64.35004526935154\n",
      "Node 3 has MSE 167.02777777777737\n",
      "Node 4 has MSE 83.96484375\n",
      "Node 5 has MSE 51.6875\n",
      "Node 6 has MSE 0.0\n",
      "Node 7 has MSE 10.489795918369055\n",
      "Node 8 has MSE 0.6666666666660603\n",
      "Node 9 has MSE 0.25\n",
      "Node 10 has MSE 0.0\n",
      "Node 11 has MSE 0.0\n",
      "Node 12 has MSE 0.0\n",
      "Node 13 has MSE 0.0\n",
      "Node 14 has MSE 54.359375\n",
      "Node 15 has MSE 15.1875\n",
      "Node 16 has MSE 5.555555555555657\n",
      "Node 17 has MSE 0.0\n",
      "Node 18 has MSE 0.0\n",
      "Node 19 has MSE 0.0\n",
      "Node 20 has MSE 18.5\n",
      "Node 21 has MSE 4.0\n",
      "Node 22 has MSE 0.0\n",
      "Node 23 has MSE 0.0\n",
      "Node 24 has MSE 1.0\n",
      "Node 25 has MSE 0.0\n",
      "Node 26 has MSE 0.0\n",
      "Node 27 has MSE 100.0\n",
      "Node 28 has MSE 0.0\n",
      "Node 29 has MSE 0.0\n",
      "Node 30 has MSE 27.533240997230678\n",
      "Node 31 has MSE 4.978395061727497\n",
      "Node 32 has MSE 5.555555555556566\n",
      "Node 33 has MSE 0.6666666666660603\n",
      "Node 34 has MSE 0.0\n",
      "Node 35 has MSE 0.25\n",
      "Node 36 has MSE 0.0\n",
      "Node 37 has MSE 0.0\n",
      "Node 38 has MSE 2.6666666666660603\n",
      "Node 39 has MSE 1.0399999999990541\n",
      "Node 40 has MSE 0.25\n",
      "Node 41 has MSE 0.0\n",
      "Node 42 has MSE 0.0\n",
      "Node 43 has MSE 0.22222222222080745\n",
      "Node 44 has MSE 0.0\n",
      "Node 45 has MSE 0.0\n",
      "Node 46 has MSE 0.0\n",
      "Node 47 has MSE 0.5432098765413684\n",
      "Node 48 has MSE 0.0\n",
      "Node 49 has MSE 0.40816326530512015\n",
      "Node 50 has MSE 0.1599999999980355\n",
      "Node 51 has MSE 0.0\n",
      "Node 52 has MSE 0.0\n",
      "Node 53 has MSE 0.0\n",
      "Node 54 has MSE 30.51516052318766\n",
      "Node 55 has MSE 26.739691795084582\n",
      "Node 56 has MSE 22.976912630148945\n",
      "Node 57 has MSE 20.400000000001455\n",
      "Node 58 has MSE 23.609375\n",
      "Node 59 has MSE 5.109375\n",
      "Node 60 has MSE 1.555555555556566\n",
      "Node 61 has MSE 2.5600000000013097\n",
      "Node 62 has MSE 17.609375\n",
      "Node 63 has MSE 7.265306122448237\n",
      "Node 64 has MSE 0.0\n",
      "Node 65 has MSE 12.349583828774485\n",
      "Node 66 has MSE 0.0\n",
      "Node 67 has MSE 7.616071428570649\n",
      "Node 68 has MSE 6.5123966942155675\n",
      "Node 69 has MSE 3.7716262975773134\n",
      "Node 70 has MSE 6.25\n",
      "Node 71 has MSE 0.0\n",
      "Node 72 has MSE 0.0\n",
      "Node 73 has MSE 0.0\n",
      "Node 74 has MSE 22.222222222221717\n",
      "Node 75 has MSE 10.159999999999854\n",
      "Node 76 has MSE 0.25\n",
      "Node 77 has MSE 0.0\n",
      "Node 78 has MSE 0.0\n",
      "Node 79 has MSE 1.5555555555547471\n",
      "Node 80 has MSE 0.0\n",
      "Node 81 has MSE 0.25\n",
      "Node 82 has MSE 0.0\n",
      "Node 83 has MSE 0.0\n",
      "Node 84 has MSE 2.1875\n",
      "Node 85 has MSE 0.0\n",
      "Node 86 has MSE 0.6666666666669698\n",
      "Node 87 has MSE 0.0\n",
      "Node 88 has MSE 0.25\n",
      "Node 89 has MSE 0.0\n",
      "Node 90 has MSE 0.0\n",
      "Node 91 has MSE 1806.25\n",
      "Node 92 has MSE 0.0\n",
      "Node 93 has MSE 0.0\n",
      "Node 94 has MSE 0.0\n"
     ]
    }
   ],
   "source": [
    "extracted_MSEs = regressor.tree_.impurity   \n",
    "for idx, MSE in enumerate(regressor.tree_.impurity):\n",
    "    print(\"Node {} has MSE {}\".format(idx,MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hw.pdf'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting the Tree \n",
    "dot_data = tree.export_graphviz(regressor, out_file=None, feature_names=temp_df.columns[1:-1])\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"hw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: 7.8917893961708385\n",
      "MSE TEST: 118.43176275510203\n",
      "R2 Train: 0.9517086472302221\n",
      "R2 TEST: -0.05492477441978694\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "y_train_pred = regressor.predict(X_train)\n",
    "y_test_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculation of Mean Squared Error (MSE)\n",
    "print(\"MSE Train:\", mean_squared_error(y_train,y_train_pred))\n",
    "print(\"MSE TEST:\", mean_squared_error(y_test,y_test_pred))\n",
    "\n",
    "print(\"R2 Train:\", r2_score(y_train,y_train_pred))\n",
    "print(\"R2 TEST:\", r2_score(y_test,y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
